import numpy  as np
import pandas as pd
import cv2
import os
import sys
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import timm
import glob
import torch
from collections import Counter
import textwrap

AGD20K_PATH = '/home/DATA/AGD20K'
# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ ìƒìœ„ í´ë”ë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ------------------------------------------------------
# 3. Local Modules (ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ)
# ------------------------------------------------------
# ê²½ë¡œ ì„¤ì •ì´ ì™„ë£Œëœ í›„ import í•´ì•¼ í•©ë‹ˆë‹¤.
from VLM_model_dot_relative import MetricsTracker
from file_managing import (
    load_selected_samples,
    get_actual_path,
    get_gt_path,
    prompt_dict_obj
)


from scipy.stats import pearsonr
from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation

processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
clip_model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined")


def get_clipseg_heatmap(
        image_path: str,
        model, 
        processor, 
        object_name: str,
    ):
    """
    (ìˆ˜ì •ë¨) CLIPSeg ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê°„ì˜
    ì„¸ê·¸ë©˜í…Œì´ì…˜ íˆíŠ¸ë§µì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if model is None or processor is None:
        print("Error: CLIPSeg model or processor not loaded.")
        return None, None
    
    original_image = Image.open(image_path).convert('RGB')
    original_size = original_image.size # (width, height)

    # 1. ë‹¨ì¼ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt_text = object_name

    # 2. ì…ë ¥ ì²˜ë¦¬
    inputs = processor(
        text=[prompt_text], 
        images=[original_image], 
        padding="max_length", 
        return_tensors="pt"
    )
    
    # 3. ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(**inputs)
        # predsì˜ shape ì²˜ë¦¬ëŠ” ë¡œì§ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ê²°ê³¼ì ìœ¼ë¡œ heatmapì„ ë½‘ì„ ë•Œ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
        preds = outputs.logits.unsqueeze(0).unsqueeze(1) 

    # 4. íˆíŠ¸ë§µ ìƒì„±
    # [ì¤‘ìš” ìˆ˜ì •] .squeeze()ë¥¼ ì¶”ê°€í•˜ì—¬ (1, 352, 352) -> (352, 352)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    heatmap_small = torch.sigmoid(preds[0][0]).cpu().detach().squeeze() 

    # 5. PIL ì´ë¯¸ì§€ ë³€í™˜ ë° ë¦¬ì‚¬ì´ì¦ˆ
    # heatmap_small.numpy()ëŠ” ì´ì œ (352, 352)ì´ë¯€ë¡œ PILì´ ì •ìƒì ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.
    # float32 íƒ€ì… ìœ ì§€ë¥¼ ìœ„í•´ mode='F'ë¥¼ ëª…ì‹œí•  ìˆ˜ë„ ìˆìœ¼ë‚˜, ë³´í†µ ê·¸ëƒ¥ ë„˜ê²¨ë„ ë©ë‹ˆë‹¤.
    final_heatmap = np.array(
        Image.fromarray(heatmap_small.numpy())
        .resize(original_size, resample=Image.Resampling.BILINEAR)
    )
    
    # print(f"shape of final_heatmap : {final_heatmap.shape}")

    # 0-1 ì •ê·œí™”
    if final_heatmap.max() > 0:
        final_heatmap = (final_heatmap - final_heatmap.min()) / (final_heatmap.max() - final_heatmap.min())
        # gamma, epsilonì€ ì™¸ë¶€ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í•¨ìˆ˜ ì¸ìë¡œ ë°›ê±°ë‚˜ ì „ì—­ ë³€ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì½”ë“œ ë§¥ë½ìƒ ì „ì—­ ë³€ìˆ˜ gamma, epsilonì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        final_heatmap = final_heatmap ## ** gamma ##+ epsilon
        
    return final_heatmap


df_attention = pd.read_pickle("attention_result_32B.pkl")
print(f"Length of dataset : {len(df_attention)}")
top_10_frequency_counter = Counter()

head_performance = {}

for index, row in df_attention.iterrows():
    object_name = row['object']
    action = row['action']
    filename = row['filename']
    description = row['description']
    attention_value = row['s_img']
    file_name_real = f"{AGD20K_PATH}/Seen/testset/egocentric/{action}/{object_name}/{filename}"
    print(f"Processing image {index}, object : {object_name}, action : {action}")

    clip_heatmap = get_clipseg_heatmap(
        file_name_real,
        clip_model, # Pass the model object (now on GPU)
        processor,
        object_name,
    )
    # 2. CLIPSeg íˆíŠ¸ë§µì„ ì–´í…ì…˜ ë§µ í¬ê¸°(31x31)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ë¹„êµë¥¼ ìœ„í•´)
    clip_heatmap_resized = cv2.resize(clip_heatmap, (31, 31), interpolation=cv2.INTER_LINEAR)
    clip_flat = clip_heatmap_resized.flatten()
    current_image_scores = []

    for idx in attention_value: 
        layer = idx['layer']
        head = idx['head']
        if layer == 26 and head == 20:
            inside_heatmap = idx['heatmap']
            
            # 1. ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ (OpenCVëŠ” BGRì´ë¯€ë¡œ RGBë¡œ ë³€í™˜)
            orig_img = cv2.imread(file_name_real)
            orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)
            h, w, _ = orig_img.shape

            # 2. ì–´í…ì…˜ íˆíŠ¸ë§µ ì „ì²˜ë¦¬ ë° ë¦¬ì‚¬ì´ì¦ˆ
            # 0~1 ì‚¬ì´ë¡œ ì •ê·œí™” (ì´ë¯¸ ë˜ì–´ìˆì„ ìˆ˜ ìˆì§€ë§Œ ì•ˆì „ì„ ìœ„í•´)
            attn_norm = (inside_heatmap - inside_heatmap.min()) / (inside_heatmap.max() - inside_heatmap.min() + 1e-8)
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¸í„°í´ë ˆì´ì…˜ (BILINEAR ì¶”ì²œ)
            attn_resized = cv2.resize(attn_norm, (w, h), interpolation=cv2.INTER_LINEAR)

            # 3. íˆíŠ¸ë§µ ì»¬ëŸ¬ë§µ ì ìš© (JET ì»¬ëŸ¬ë§µ ì‚¬ìš©)
            heatmap_color = cv2.applyColorMap(np.uint8(255 * attn_resized), cv2.COLORMAP_JET)
            heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB) # RGB ë³€í™˜

            # 4. í•©ì„± ì´ë¯¸ì§€ ìƒì„± (ì›ë³¸ 60% + íˆíŠ¸ë§µ 40%)
            overlay_img = cv2.addWeighted(orig_img, 0.6, heatmap_color, 0.4, 0)

            # 4. ì‹œê°í™” ì„¤ì • (figsizeë¥¼ ì¡°ê¸ˆ í‚¤ì›Œ í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´)
            fig, axes = plt.subplots(1, 3, figsize=(22, 10))
            
            # ìƒë‹¨ ì œëª© ì •ë³´ êµ¬ì„± (descriptionì€ 100ìë§ˆë‹¤ ì¤„ë°”ê¿ˆ)
            wrapped_desc = "\n".join(textwrap.wrap(f"Description: {description}", width=120))
            full_title = (
                f"Object: {object_name}  |  Action: {action}  |  File: {filename}\n"
                f"{wrapped_desc}"
            )
            
            # ì „ì²´ ì œëª© ì¶”ê°€ (í°íŠ¸ í¬ê¸° ë° ìœ„ì¹˜ ì¡°ì •)
            plt.suptitle(full_title, fontsize=15, fontweight='bold', y=0.95)

            # ê° ì„œë¸Œí”Œë¡¯ í‘œì‹œ
            axes[0].imshow(orig_img)
            axes[0].set_title("Original Image", fontsize=12)
            axes[0].axis('off')

            axes[1].imshow(heatmap_color)
            axes[1].set_title(f"Attention Heatmap (Layer {layer} Head {head})", fontsize=12)
            axes[1].axis('off')

            axes[2].imshow(overlay_img)
            axes[2].set_title("Overlay (Attention + Image)", fontsize=12)
            axes[2].axis('off')

            # 5. ì—¬ë°± ì¡°ì • ë° ì €ì¥
            plt.subplots_adjust(top=0.82) # ì œëª©ì´ ì´ë¯¸ì§€ì™€ ê²¹ì¹˜ì§€ ì•Šê²Œ ìƒë‹¨ ì—¬ë°± ì¡°ì ˆ
            
            save_dir = "vis_results_L26_H20"
            os.makedirs(save_dir, exist_ok=True)
            save_path = f"{save_dir}/{object_name}_{action}_{filename}.png"
            plt.savefig(save_path, bbox_inches='tight', dpi=150)
            plt.close()

            print(f"ğŸ“¸ ì‹œê°í™” ì™„ë£Œ: {save_path}")