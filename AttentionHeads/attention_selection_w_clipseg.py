import numpy  as np
import pandas as pd
import cv2
import os
import sys
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import timm
import glob
import torch
from collections import Counter

AGD20K_PATH = '/home/DATA/AGD20K'
# í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ ê¸°ì¤€ ìƒìœ„ í´ë”ë¥¼ ì‹œìŠ¤í…œ ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

# ------------------------------------------------------
# 3. Local Modules (ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ)
# ------------------------------------------------------
# ê²½ë¡œ ì„¤ì •ì´ ì™„ë£Œëœ í›„ import í•´ì•¼ í•©ë‹ˆë‹¤.
from VLM_model_dot_relative import MetricsTracker
from file_managing import (
    load_selected_samples,
    get_actual_path,
    get_gt_path,
    prompt_dict_obj
)


from scipy.stats import pearsonr
from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation

processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
clip_model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined")


def get_clipseg_heatmap(
        image_path: str,
        model, 
        processor, 
        object_name: str,
    ):
    """
    (ìˆ˜ì •ë¨) CLIPSeg ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê°„ì˜
    ì„¸ê·¸ë©˜í…Œì´ì…˜ íˆíŠ¸ë§µì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    if model is None or processor is None:
        print("Error: CLIPSeg model or processor not loaded.")
        return None, None
    
    original_image = Image.open(image_path).convert('RGB')
    original_size = original_image.size # (width, height)

    # 1. ë‹¨ì¼ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt_text = object_name

    # 2. ì…ë ¥ ì²˜ë¦¬
    inputs = processor(
        text=[prompt_text], 
        images=[original_image], 
        padding="max_length", 
        return_tensors="pt"
    )
    
    # 3. ì˜ˆì¸¡
    with torch.no_grad():
        outputs = model(**inputs)
        # predsì˜ shape ì²˜ë¦¬ëŠ” ë¡œì§ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ê²°ê³¼ì ìœ¼ë¡œ heatmapì„ ë½‘ì„ ë•Œ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
        preds = outputs.logits.unsqueeze(0).unsqueeze(1) 

    # 4. íˆíŠ¸ë§µ ìƒì„±
    # [ì¤‘ìš” ìˆ˜ì •] .squeeze()ë¥¼ ì¶”ê°€í•˜ì—¬ (1, 352, 352) -> (352, 352)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    heatmap_small = torch.sigmoid(preds[0][0]).cpu().detach().squeeze() 

    # 5. PIL ì´ë¯¸ì§€ ë³€í™˜ ë° ë¦¬ì‚¬ì´ì¦ˆ
    # heatmap_small.numpy()ëŠ” ì´ì œ (352, 352)ì´ë¯€ë¡œ PILì´ ì •ìƒì ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.
    # float32 íƒ€ì… ìœ ì§€ë¥¼ ìœ„í•´ mode='F'ë¥¼ ëª…ì‹œí•  ìˆ˜ë„ ìˆìœ¼ë‚˜, ë³´í†µ ê·¸ëƒ¥ ë„˜ê²¨ë„ ë©ë‹ˆë‹¤.
    final_heatmap = np.array(
        Image.fromarray(heatmap_small.numpy())
        .resize(original_size, resample=Image.Resampling.BILINEAR)
    )
    
    # print(f"shape of final_heatmap : {final_heatmap.shape}")

    # 0-1 ì •ê·œí™”
    if final_heatmap.max() > 0:
        final_heatmap = (final_heatmap - final_heatmap.min()) / (final_heatmap.max() - final_heatmap.min())
        # gamma, epsilonì€ ì™¸ë¶€ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í•¨ìˆ˜ ì¸ìë¡œ ë°›ê±°ë‚˜ ì „ì—­ ë³€ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ì½”ë“œ ë§¥ë½ìƒ ì „ì—­ ë³€ìˆ˜ gamma, epsilonì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
        final_heatmap = final_heatmap ## ** gamma ##+ epsilon
        
    return final_heatmap

def calculate_attention_ratio(heatmap_31x31, binary_mask_original):
    """
    (ë§ˆìŠ¤í¬ ë‚´ë¶€ ì–´í…ì…˜ í•©) / (ì „ì²´ ì–´í…ì…˜ í•©) ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    # 1. ë°”ì´ë„ˆë¦¬ ë§ˆìŠ¤í¬ë¥¼ ì–´í…ì…˜ í¬ê¸°(31x31)ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    # ì¸í„°í´ë ˆì´ì…˜ì€ INTER_NEARESTë¥¼ ì¨ì„œ 0ê³¼ 1ì˜ ê²½ê³„ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì •í™•í•©ë‹ˆë‹¤.
    mask_resized = cv2.resize(binary_mask_original, (31, 31), interpolation=cv2.INTER_NEAREST)
    
    # 2. ë§ˆìŠ¤í¬ ë‚´ë¶€ ì–´í…ì…˜ í•© ê³„ì‚°
    # ë‘ í–‰ë ¬ì„ ê³±í•˜ë©´ ë§ˆìŠ¤í¬ê°€ 0ì¸ ë¶€ë¶„ì˜ ì–´í…ì…˜ì€ ëª¨ë‘ 0ì´ ë©ë‹ˆë‹¤.
    inside_sum = np.sum(heatmap_31x31 * mask_resized)
    
    # 3. ì „ì²´ ì–´í…ì…˜ í•© ê³„ì‚°
    total_sum = np.sum(heatmap_31x31)
    
    # 4. ë¹„ìœ¨ ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
    ratio = (inside_sum / total_sum) if total_sum > 0 else 0
    return ratio


df_attention = pd.read_pickle("attention_result_32B_simple_description.pkl")
print(f"Length of dataset : {len(df_attention)}")
top_10_frequency_counter = Counter()

head_performance = {}

for index, row in df_attention.iterrows():
    object_name = row['object']
    action = row['action']
    filename = row['filename']
    # description = row['description']
    attention_value = row['s_img']
    file_name_real = f"{AGD20K_PATH}/Seen/testset/egocentric/{action}/{object_name}/{filename}"
    print(f"Processing image {index}, object : {object_name}, action : {action}")

    clip_heatmap = get_clipseg_heatmap(
        file_name_real,
        clip_model, # Pass the model object (now on GPU)
        processor,
        object_name,
    )
    clip_binary_mask = (clip_heatmap > 0.15).astype(np.float32)
    
    # 2. CLIPSeg resize as same as Qwen3 path  size (31x31)
    # clip_heatmap_resized = cv2.resize(clip_heatmap, (31, 31), interpolation=cv2.INTER_LINEAR)
    clip_heatmap_resized = cv2.resize(clip_binary_mask, (31, 31), interpolation=cv2.INTER_LINEAR)
    
    clip_flat = clip_heatmap_resized.flatten()
    
    current_image_scores = []

    for idx in attention_value: 
        layer = idx['layer']
        head = idx['head']
        inside_heatmap = idx['heatmap']
        inside_flat = inside_heatmap.flatten()
        # print(f"layer : {idx['layer']}, head : {idx['head']} , S_img : {idx['S_img']}")

        # score, _ = pearsonr(inside_flat, clip_flat)
        

        threshold = np.percentile(inside_flat, 97)
        filtered_heatmap = np.where(inside_heatmap >= threshold, inside_heatmap, 0)
        filtered_inside_flat = filtered_heatmap.flatten()

        # score = calculate_attention_ratio(inside_heatmap, clip_binary_mask)
        # score = calculate_attention_ratio(filtered_heatmap, clip_binary_mask)
        score, _ = pearsonr(filtered_inside_flat, clip_flat)

        current_image_scores.append({
            'key': (layer, head),
            'score': score
        })


        # 2. í˜„ì¬ ì´ë¯¸ì§€ì—ì„œ ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ Top 10 ì„ ì •
        top_10_heads = sorted(current_image_scores, key=lambda x: x['score'], reverse=True)[:10]

        # 3. ì„ ì •ëœ Top 10 í—¤ë“œì˜ ë¹ˆë„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        for head_info in top_10_heads:
            top_10_frequency_counter[head_info['key']] += 1
     
# ------------------------------------------------------
# 2. ê²°ê³¼ ì§‘ê³„ ë° ì¶œë ¥
# ------------------------------------------------------
print("\n" + "="*50)
print("ğŸ† ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ Top 10ì— ì„ ì •ëœ í—¤ë“œ 5ê°œ")
print("="*50)

# ë¹ˆë„ ìˆœìœ¼ë¡œ ìƒìœ„ 5ê°œ ì¶”ì¶œ
most_common_5 = top_10_frequency_counter.most_common(5)

for rank, (key, count) in enumerate(most_common_5, 1):
    layer, head = key
    percentage = (count / (len(df_attention)*4096)) * 100
    print(f"Top {rank} | Layer: {layer:2d}, Head: {head:2d} | ì„ ì • íšŸìˆ˜: {count:3d}íšŒ (ì „ì²´ì˜ {percentage:.1f}%)")

print("="*50)

# ==================================================
# ğŸ† ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ Top 10ì— ì„ ì •ëœ í—¤ë“œ 5ê°œ - clipseg pearson ì‚¬ìš©
# ==================================================
# Top 1 | Layer: 26, Head: 20 | ì„ ì • íšŸìˆ˜: 90556íšŒ (ì „ì²´ì˜ 74839.7%)
# Top 2 | Layer: 26, Head: 33 | ì„ ì • íšŸìˆ˜: 54266íšŒ (ì „ì²´ì˜ 44847.9%)
# Top 3 | Layer: 24, Head: 31 | ì„ ì • íšŸìˆ˜: 53235íšŒ (ì „ì²´ì˜ 43995.9%)
# Top 4 | Layer: 21, Head: 38 | ì„ ì • íšŸìˆ˜: 51845íšŒ (ì „ì²´ì˜ 42847.1%)
# Top 5 | Layer: 17, Head: 28 | ì„ ì • íšŸìˆ˜: 49085íšŒ (ì „ì²´ì˜ 40566.1%)
# ==================================================


# ==================================================  clipseg pearson ì‚¬ìš©+ top3
# ğŸ† ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ Top 10ì— ì„ ì •ëœ í—¤ë“œ 5ê°œ
# ==================================================
# Top 1 | Layer: 26, Head: 20 | ì„ ì • íšŸìˆ˜: 78284íšŒ (ì „ì²´ì˜ 15.8%)
# Top 2 | Layer: 22, Head: 26 | ì„ ì • íšŸìˆ˜: 65322íšŒ (ì „ì²´ì˜ 13.2%)
# Top 3 | Layer:  0, Head: 20 | ì„ ì • íšŸìˆ˜: 58491íšŒ (ì „ì²´ì˜ 11.8%)
# Top 4 | Layer: 24, Head: 31 | ì„ ì • íšŸìˆ˜: 55537íšŒ (ì „ì²´ì˜ 11.2%)
# Top 5 | Layer:  0, Head: 17 | ì„ ì • íšŸìˆ˜: 51190íšŒ (ì „ì²´ì˜ 10.3%)
# ==================================================


# ==================================================  clipseg pearson ì‚¬ìš©+ top3 + simple description
# ğŸ† ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ Top 10ì— ì„ ì •ëœ í—¤ë“œ 5ê°œ
# ==================================================
# Top 1 | Layer: 15, Head:  1 | ì„ ì • íšŸìˆ˜: 93077íšŒ (ì „ì²´ì˜ 18.8%)
# Top 2 | Layer: 14, Head:  6 | ì„ ì • íšŸìˆ˜: 68714íšŒ (ì „ì²´ì˜ 13.9%)
# Top 3 | Layer: 28, Head: 42 | ì„ ì • íšŸìˆ˜: 65578íšŒ (ì „ì²´ì˜ 13.2%)
# Top 4 | Layer: 24, Head: 31 | ì„ ì • íšŸìˆ˜: 56458íšŒ (ì „ì²´ì˜ 11.4%)
# Top 5 | Layer: 16, Head: 62 | ì„ ì • íšŸìˆ˜: 55598íšŒ (ì „

# ==================================================
# ğŸ† ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ Top 10ì— ì„ ì •ëœ í—¤ë“œ 5ê°œ  - ClipSeg ì—ì„œ binary mask ì‚¬ìš©
# ==================================================
# Top 1 | Layer:  0, Head: 20 | ì„ ì • íšŸìˆ˜: 104239íšŒ (ì „ì²´ì˜ 21.0%)
# Top 2 | Layer: 15, Head: 12 | ì„ ì • íšŸìˆ˜: 93906íšŒ (ì „ì²´ì˜ 18.9%)
# Top 3 | Layer: 15, Head: 34 | ì„ ì • íšŸìˆ˜: 92745íšŒ (ì „ì²´ì˜ 18.7%)
# Top 4 | Layer:  0, Head: 17 | ì„ ì • íšŸìˆ˜: 87156íšŒ (ì „ì²´ì˜ 17.6%)
# Top 5 | Layer: 16, Head: 59 | ì„ ì • íšŸìˆ˜: 73930íšŒ (ì „ì²´ì˜ 14.9%)


# ==================================================
# ğŸ† ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ Top 10ì— ì„ ì •ëœ í—¤ë“œ 5ê°œ  - ClipSeg ì—ì„œ binary mask ì‚¬ìš© + ìƒìœ„ 3í”„ë¡œì— ëŒ€í•´ì„œë§Œ!!
# ==================================================
# Top 1 | Layer:  0, Head: 20 | ì„ ì • íšŸìˆ˜: 109455íšŒ (ì „ì²´ì˜ 22.1%)
# Top 2 | Layer:  0, Head: 17 | ì„ ì • íšŸìˆ˜: 97219íšŒ (ì „ì²´ì˜ 19.6%)
# Top 3 | Layer: 12, Head: 25 | ì„ ì • íšŸìˆ˜: 76544íšŒ (ì „ì²´ì˜ 15.4%)
# Top 4 | Layer: 26, Head: 20 | ì„ ì • íšŸìˆ˜: 70693íšŒ (ì „ì²´ì˜ 14.3%)
# Top 5 | Layer: 22, Head: 26 | ì„ ì • íšŸìˆ˜: 60520íšŒ (ì „ì²´ì˜ 12.2%)