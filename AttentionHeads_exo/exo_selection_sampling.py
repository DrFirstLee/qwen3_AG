import os
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

import gc
import numpy as np
import pandas as pd
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from file_managing import make_input_image, make_input_image_exo
from config import AGD20K_PATH, model_name

from pathlib import Path
import copy
# ------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ------------------------------------------------------
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_ENABLE_SDPA"] = "1"

print(f"ğŸ¤– {model_name} ëª¨ë¸ ë¡œë”©ì¤‘...")

# Qwen3 Model & Processor ë¡œë“œ
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager", # Attention Map ì¶”ì¶œì„ ìœ„í•´ eager ëª¨ë“œ í•„ìˆ˜
    device_map="cuda", 
)
processor = AutoProcessor.from_pretrained(model_name)
device = model.device

# ------------------------------------------------------
# 2. ì‹¤í–‰ë¶€
# ------------------------------------------------------
system_prompt = (
    "You are a helpful language and vision assistant. "
    "You are able to understand the visual content that the user provides, "
    "and assist the user with a variety of tasks using natural language."
)

df_fin = pd.read_pickle("target_df_w_random_exo.pkl")
df_fin['action$object'] = df_fin['action'] + "$" + df_fin['object']
target_item_dict = {
        "1":"push$bicycle",
        "2" : "hold$badminton_racket",
        "3" : "hold$axe"
    }
valid_ext = {".jpg", ".PNG", ".png", ".JPG"}
df_fin = df_fin[df_fin['action$object'].isin(target_item_dict.values())]

# ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
all_inference_results = []
total_processed_count = 0 # ì¤‘ê°„ ì €ì¥ì„ ìœ„í•œ ì¹´ìš´í„°

print(f"length of Data : {len(df_fin)}")

for index, row in df_fin.iterrows():
    object_name = row['object']
    action = row['action']
    filename = row['filename']
    action_object = row['action$object']
    
    


    file_name_real = f"{AGD20K_PATH}/Seen/testset/egocentric/{action}/{object_name}/{filename}"
    exo_base_path = Path(f"{AGD20K_PATH}/Seen/trainset/exocentric/{action}/{object_name}")
    all_exo_images = [p for p in exo_base_path.rglob("*") if p.suffix.lower() in valid_ext]
    if  len(all_exo_images)==0:
        raise ValueError("No valid exo images found")

    print(f"\n{index} >>> {object_name} | {action} | Exo Images Count: {len(all_exo_images)}")

    for exo_idx, exo_path_obj in enumerate(all_exo_images):
        exo_file_real_str = str(exo_path_obj)
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        description = f"""Refer to the second image (exocentric view) for context. 
        Based on the first image (egocentric view), when people perform {action} with {object_name}, which part of the {object_name} is used for '{action}'?
        Answer in one sentence."""

        image_base64 = make_input_image_exo(file_name_real)
        exo_image_base64 = make_input_image_exo(exo_file_real_str)

        messages = [
            {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
            {"role": "user", "content": [
                {"type": "image", "image": f"data:image/jpeg;base64,{image_base64}"},
                {"type": "image", "image": f"data:image/jpeg;base64,{exo_image_base64}"},
                {"type": "text", "text": description}, 
            ]}
        ]

        # -------------------------------------------------------
        # STEP 1: Pre-process Inputs
        # -------------------------------------------------------
        inputs = processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True, # ë‹µë³€ ìƒì„±ì„ ìœ„í•´ Trueë¡œ ë³€ê²½
            return_dict=True,
            return_tensors="pt"
        ).to(device)

        # -------------------------------------------------------
        # STEP 2: Generate Output Sentence (ë‹µë³€ ìƒì„±)
        # -------------------------------------------------------
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=1024, # í•„ìš”í•œ ë§Œí¼ ì¡°ì ˆ
                do_sample=False,    # Deterministic
                use_cache=True
            )
        
        # ìƒì„±ëœ ë‹µë³€ë§Œ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
        input_len = inputs.input_ids.shape[1]
        output_ids = generated_ids[0][input_len:] # ìˆœìˆ˜ ìƒì„±ëœ í† í° IDë“¤
        output_text = processor.decode(output_ids, skip_special_tokens=True)

        print(f"   [{exo_idx+1}/{len(all_exo_images)}] Output: {output_text}")
        
        # -------------------------------------------------------
        # STEP 3: Forward Pass with Full Sequence (Attention ì¶”ì¶œ)
        # -------------------------------------------------------
        # ìƒì„±ëœ ì „ì²´ ì‹œí€€ìŠ¤(Input + Output)ë¥¼ ë‹¤ì‹œ ëª¨ë¸ì— ë„£ì–´ Attention Mapì„ êµ¬í•¨
        # QwenVLì€ image inputs(pixel_values ë“±)ì´ í•„ìš”í•˜ë¯€ë¡œ inputs ì •ë³´ ì¬ì‚¬ìš©
        
        full_input_ids = generated_ids # [1, seq_len]
        
        # inputsì— ìˆëŠ” ì´ë¯¸ì§€ ê´€ë ¨ í…ì„œë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, input_idsë§Œ ì „ì²´ ì‹œí€€ìŠ¤ë¡œ êµì²´
        # ì£¼ì˜: Qwen3VLForConditionalGenerationì˜ forward ì¸ìì— ë§ì¶° ì „ë‹¬
        with torch.no_grad():
            outputs = model(
                input_ids=full_input_ids,
                pixel_values=inputs.pixel_values,
                image_grid_thw=inputs.image_grid_thw,
                attention_mask=torch.ones_like(full_input_ids), # ì „ì²´ ë‹¤ ë³´ì´ê²Œ
                output_attentions=True,
                return_dict=True
            )
        
        attentions = outputs.attentions # Tuple of (batch, num_heads, seq_len, seq_len)
        
        # -------------------------------------------------------
        # STEP 4: Index Parsing
        # -------------------------------------------------------
        ids_list = full_input_ids[0].tolist()
        tok = processor.tokenizer
        
        # Vision Token ID ì •ì˜
        vision_start_id = tok.convert_tokens_to_ids("<|vision_start|>")
        vision_end_id   = tok.convert_tokens_to_ids("<|vision_end|>")

        # 1. ì‹œí€€ìŠ¤ ë‚´ì˜ 'ëª¨ë“ ' ì´ë¯¸ì§€ ì‹œì‘/ë ìœ„ì¹˜ ì°¾ê¸°
        all_starts = [i for i, x in enumerate(ids_list) if x == vision_start_id]
        all_ends   = [i for i, x in enumerate(ids_list) if x == vision_end_id]

    # [ìˆ˜ì •] ì´ë¯¸ì§€ 2ê°œê°€ ì •ìƒì ìœ¼ë¡œ ìˆëŠ”ì§€ í™•ì¸
        if len(all_starts) < 2:
            raise Exception("Image not found in sequence.")

        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ (Egocentric) ë²”ìœ„
        idx1_start, idx1_end = all_starts[0], all_ends[0]
        # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ (Exocentric) ë²”ìœ„
        idx2_start, idx2_end = all_starts[1], all_ends[1]

        # íˆíŠ¸ë§µ ìƒì„±ì„ ìœ„í•œ Target(Ego) ì„¤ì •
        target_start = idx1_start
        target_end = idx1_end

        # Grid Info (ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê¸°ì¤€)
        grid_t, grid_h, grid_w = inputs.image_grid_thw[0].detach().cpu().numpy()
        llm_grid_h = grid_h // 2
        llm_grid_w = grid_w // 2
        query_start_idx = input_len
        query_end_idx = len(ids_list)  
        output_attn_data = []

        # ê° Output Tokenì— ëŒ€í•´ ìˆœíšŒ (Query Iteration)
        for q_idx in range(query_start_idx, query_end_idx):
            token_id = ids_list[q_idx]
            token_str = tok.decode([token_id])
            
            token_data = {
                "token_idx_in_seq": q_idx,
                "token_str": token_str,
                "token_id": token_id,
                "attentions": [] # ê° ë ˆì´ì–´/í—¤ë“œì˜ heatmap ì •ë³´
            }

            # ëª¨ë“  Layer ìˆœíšŒ
            for layer_idx, layer_attn in enumerate(attentions):
                # layer_attn shape: [batch, num_heads, seq_len, seq_len]
                # [0, :, q_idx, :] -> í˜„ì¬ í† í°(q_idx)ì´ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë°”ë¼ë³´ëŠ” Attention
                
                heads_attn = layer_attn[0, :, q_idx, :] # [num_heads, seq_len]
                num_heads = heads_attn.shape[0]

                for head_idx in range(num_heads):
                    this_head_attn = heads_attn[head_idx] # [seq_len]
                    
                    # [ìˆ˜ì •] 1ë²ˆ ì´ë¯¸ì§€ Attention í•©
                    attn_img1 = this_head_attn[idx1_start + 1 : idx1_end]
                    sum_img1 = float(attn_img1.sum().detach().cpu().item())
                    # [ìˆ˜ì •] 2ë²ˆ ì´ë¯¸ì§€ Attention í•© (ì¡´ì¬í•  ê²½ìš°)
                    sum_img2 = 0.0
                    attn_img2 = this_head_attn[idx2_start + 1 : idx2_end]
                    sum_img2 = float(attn_img2.sum().detach().cpu().item())
                    # [ìµœì¢…] ë‘ ì´ë¯¸ì§€ì˜ Attention ì´í•©
                    total_s_img_val = sum_img1 + sum_img2

                    # Heatmap ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ float16 ë“±ìœ¼ë¡œ ë³€í™˜ ê³ ë ¤ ê°€ëŠ¥)
                    heatmap_np = attn_img1.reshape(llm_grid_h, llm_grid_w).float().cpu().numpy()
                    
                    # í•„ìš”í•œ ì •ë³´ë§Œ ì €ì¥ (ì „ì²´ ë§µì„ ë‹¤ ì €ì¥í•˜ë©´ ìš©ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤!)
                    # ì—¬ê¸°ì„œëŠ” ìš”ì²­ëŒ€ë¡œ "V ê°’ì„ ëª¨ë‘ ì €ì¥" í•˜ë„ë¡ heatmapì„ ì €ì¥í•©ë‹ˆë‹¤.
                    token_data["attentions"].append({
                        "layer": layer_idx,
                        "head": head_idx,
                        "s_img": total_s_img_val, # <--- ì—¬ê¸°ê°€ ìˆ˜ì •ë¨ (1+2 í•©)
                        "s_img_ego": sum_img1,    # (ì˜µì…˜) ë‚˜ì¤‘ì„ ìœ„í•´ ë¶„ë¦¬í•´ì„œ ì €ì¥í•´ë„ ì¢‹ìŒ
                        "s_img_exo": sum_img2,    # (ì˜µì…˜) 
                        "heatmap": heatmap_np 
                    })
            
            output_attn_data.append(token_data)

        # -------------------------------------------------------
        # [ê²°ê³¼ ì €ì¥] Dictionary ìƒì„± í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        # -------------------------------------------------------
        result_row = row.to_dict() # ì›ë³¸ row ì •ë³´ ë³µì‚¬
        result_row['exo_filename'] = exo_file_real_str # í˜„ì¬ Exo íŒŒì¼ëª…
        result_row['output_sentence'] = output_text
        result_row['output_attentions'] = output_attn_data
        
        all_inference_results.append(result_row)
        total_processed_count += 1
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del generated_ids, full_input_ids, outputs, attentions, output_attn_data, inputs
        torch.cuda.empty_cache()
        gc.collect()


    temp_df = pd.DataFrame(all_inference_results)
    temp_df = temp_df[temp_df['action$object'] == action_object]
    temp_df.to_pickle(f"exo_sample_32B_{action_object}.pkl")
    print(f"âœ… Saved at exo_sample_32B_{action_object}.pkl")

print("ì‘ì—… ì™„ë£Œ")