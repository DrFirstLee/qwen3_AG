import os
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

import gc
import numpy as np
import pandas as pd
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from file_managing import make_input_image
from config import AGD20K_PATH, model_name

# ------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ------------------------------------------------------
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_ENABLE_SDPA"] = "1"

print(f"ğŸ¤– {model_name} ëª¨ë¸ ë¡œë”©ì¤‘...")

# Qwen3 Model & Processor ë¡œë“œ
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager", # Attention Map ì¶”ì¶œì„ ìœ„í•´ eager ëª¨ë“œ í•„ìˆ˜
    device_map="cuda", 
)
processor = AutoProcessor.from_pretrained(model_name)
device = model.device

# ------------------------------------------------------
# 2. ì‹¤í–‰ë¶€
# ------------------------------------------------------
system_prompt = (
    "You are a helpful language and vision assistant. "
    "You are able to understand the visual content that the user provides, "
    "and assist the user with a variety of tasks using natural language."
)

df_fin = pd.read_pickle("target_df_w_random_exo.pkl")
# ê²°ê³¼ë¥¼ ë‹´ì„ ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤
df_fin['output_sentence'] = "" 
df_fin['output_attentions'] = "" # ì—¬ê¸°ì— ë³µì¡í•œ êµ¬ì¡°ì˜ ë°ì´í„°ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.

print(f"length of Data : {len(df_fin)}")

for index, row in df_fin.iterrows():
    object_name = row['object']
    action = row['action']
    filename = row['filename']
    
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    description = f"""Refer to the second image (exocentric view) for context. 
    When people perform {action} with {object_name}, which part of the {object_name} is used for '{action}'?
    Answer in one sentence."""


    file_name_real = f"{AGD20K_PATH}/Seen/testset/egocentric/{action}/{object_name}/{filename}"
    exo_file_name_real = row['random_exo_filename'].replace("/home/DATA/AGD20K", AGD20K_PATH)
    print(f"\n{index} >>> {object_name} | {action} | {filename}")
    
    image_base64 = make_input_image(file_name_real)
    exo_image_base64 = make_input_image(exo_file_name_real)

    messages = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
        {"role": "user", "content": [
            {"type": "image", "image": f"data:image/jpeg;base64,{image_base64}"},
            {"type": "image", "image": f"data:image/jpeg;base64,{exo_image_base64}"},
            {"type": "text", "text": description}, 
        ]}
    ]

    # -------------------------------------------------------
    # STEP 1: Pre-process Inputs
    # -------------------------------------------------------
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True, # ë‹µë³€ ìƒì„±ì„ ìœ„í•´ Trueë¡œ ë³€ê²½
        return_dict=True,
        return_tensors="pt"
    )
    inputs = inputs.to(device)

    # -------------------------------------------------------
    # STEP 2: Generate Output Sentence (ë‹µë³€ ìƒì„±)
    # -------------------------------------------------------
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024, # í•„ìš”í•œ ë§Œí¼ ì¡°ì ˆ
            do_sample=False,    # Deterministic
            use_cache=True
        )
    
    # ìƒì„±ëœ ë‹µë³€ë§Œ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
    input_len = inputs.input_ids.shape[1]
    output_ids = generated_ids[0][input_len:] # ìˆœìˆ˜ ìƒì„±ëœ í† í° IDë“¤
    output_text = processor.decode(output_ids, skip_special_tokens=True)
    
    print(f"ğŸ“ Output Sentence: {output_text}")
    df_fin.at[index, 'output_sentence'] = output_text

    # -------------------------------------------------------
    # STEP 3: Forward Pass with Full Sequence (Attention ì¶”ì¶œ)
    # -------------------------------------------------------
    # ìƒì„±ëœ ì „ì²´ ì‹œí€€ìŠ¤(Input + Output)ë¥¼ ë‹¤ì‹œ ëª¨ë¸ì— ë„£ì–´ Attention Mapì„ êµ¬í•¨
    # QwenVLì€ image inputs(pixel_values ë“±)ì´ í•„ìš”í•˜ë¯€ë¡œ inputs ì •ë³´ ì¬ì‚¬ìš©
    
    full_input_ids = generated_ids # [1, seq_len]
    
    # inputsì— ìˆëŠ” ì´ë¯¸ì§€ ê´€ë ¨ í…ì„œë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, input_idsë§Œ ì „ì²´ ì‹œí€€ìŠ¤ë¡œ êµì²´
    # ì£¼ì˜: Qwen3VLForConditionalGenerationì˜ forward ì¸ìì— ë§ì¶° ì „ë‹¬
    with torch.no_grad():
        outputs = model(
            input_ids=full_input_ids,
            pixel_values=inputs.pixel_values,
            image_grid_thw=inputs.image_grid_thw,
            attention_mask=torch.ones_like(full_input_ids), # ì „ì²´ ë‹¤ ë³´ì´ê²Œ
            output_attentions=True,
            return_dict=True
        )
    
    attentions = outputs.attentions # Tuple of (batch, num_heads, seq_len, seq_len)
    
    # -------------------------------------------------------
    # STEP 4: Index Parsing
    # -------------------------------------------------------
    ids_list = full_input_ids[0].tolist()
    tok = processor.tokenizer
    
    # Vision Token ìœ„ì¹˜ ì°¾ê¸°
    vision_start_id = tok.convert_tokens_to_ids("<|vision_start|>")
    vision_end_id   = tok.convert_tokens_to_ids("<|vision_end|>")

    vis_start_idx = ids_list.index(vision_start_id)
    vis_end_idx   = ids_list.index(vision_end_id)


    # Output Token ìœ„ì¹˜ ë²”ìœ„ (Query ë²”ìœ„)
    # input_len ë¶€í„° ëê¹Œì§€ê°€ ìƒì„±ëœ ë‹µë³€ì˜ í† í°ë“¤ì…ë‹ˆë‹¤.
    # ë‹¨, <|endoftext|> ê°™ì€ê²Œ ë’¤ì— ë¶™ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì‹¤ì œ ìœ ì˜ë¯¸í•œ í† í°ë§Œ ë³¼ ìˆ˜ë„ ìˆìŒ.
    # ì—¬ê¸°ì„œëŠ” output_ids ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•¨.
    query_start_idx = input_len
    query_end_idx = len(ids_list) 

    # Grid Info for Reshaping
    grid_t, grid_h, grid_w = inputs.image_grid_thw[0].detach().cpu().numpy()
    llm_grid_h = grid_h // 2
    llm_grid_w = grid_w // 2

    # -------------------------------------------------------
    # STEP 5: Extract & Store Attentions
    # -------------------------------------------------------
    # ë°ì´í„° êµ¬ì¡°: List of Dicts
    # [ 
    #   { 
    #     "token_str": "Handle", 
    #     "token_id": 1234, 
    #     "layer_data": [ 
    #         { "layer": 0, "head": 0, "heatmap": (H, W) array }, ... 
    #     ]
    #   }, ...
    # ]
    
    output_attn_data = []

    # ê° Output Tokenì— ëŒ€í•´ ìˆœíšŒ (Query Iteration)
    for q_idx in range(query_start_idx, query_end_idx):
        token_id = ids_list[q_idx]
        token_str = tok.decode([token_id])
        
        token_data = {
            "token_idx_in_seq": q_idx,
            "token_str": token_str,
            "token_id": token_id,
            "attentions": [] # ê° ë ˆì´ì–´/í—¤ë“œì˜ heatmap ì •ë³´
        }

        # ëª¨ë“  Layer ìˆœíšŒ
        for layer_idx, layer_attn in enumerate(attentions):
            # layer_attn shape: [batch, num_heads, seq_len, seq_len]
            # [0, :, q_idx, :] -> í˜„ì¬ í† í°(q_idx)ì´ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë°”ë¼ë³´ëŠ” Attention
            
            heads_attn = layer_attn[0, :, q_idx, :] # [num_heads, seq_len]
            num_heads = heads_attn.shape[0]

            for head_idx in range(num_heads):
                this_head_attn = heads_attn[head_idx] # [seq_len]
                
                # Image Tokenì— ëŒ€í•œ Attentionë§Œ ì¶”ì¶œ (Key: Vision Tokens)
                # vis_start_idx + 1 ë¶€í„° vis_end_idx ì „ê¹Œì§€ê°€ ì‹¤ì œ íŒ¨ì¹˜ í† í°
                img_attn_1d = this_head_attn[vis_start_idx + 1 : vis_end_idx]
                
                # S_img (Sum) ê³„ì‚° (ì˜µì…˜)
                s_img_val = float(img_attn_1d.sum().detach().cpu().item())
                
                # Heatmap ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ float16 ë“±ìœ¼ë¡œ ë³€í™˜ ê³ ë ¤ ê°€ëŠ¥)
                heatmap_np = img_attn_1d.reshape(llm_grid_h, llm_grid_w).float().cpu().numpy()
                
                # í•„ìš”í•œ ì •ë³´ë§Œ ì €ì¥ (ì „ì²´ ë§µì„ ë‹¤ ì €ì¥í•˜ë©´ ìš©ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤!)
                # ì—¬ê¸°ì„œëŠ” ìš”ì²­ëŒ€ë¡œ "V ê°’ì„ ëª¨ë‘ ì €ì¥" í•˜ë„ë¡ heatmapì„ ì €ì¥í•©ë‹ˆë‹¤.
                token_data["attentions"].append({
                    "layer": layer_idx,
                    "head": head_idx,
                    "s_img": s_img_val, 
                    "heatmap": heatmap_np 
                })
        
        output_attn_data.append(token_data)

    df_fin.at[index, 'output_attentions'] = output_attn_data
    
    # -------------------------------------------------------
    # STEP 6: Save & Cleanup
    # -------------------------------------------------------
    save_every = 1 # ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ë” ìì£¼ ì €ì¥ ê¶Œì¥
    if index % save_every == 0:
        # ìš©ëŸ‰ ë¬¸ì œë¡œ ë¶„í•  ì €ì¥í•˜ê±°ë‚˜, í•„ìš”í•œ í†µê³„ëŸ‰ë§Œ ë‚¨ê¸°ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤.
        df_fin.to_pickle(f"exo_attention_result_2B.pkl")
        print(f"âœ… Saved at index={index}")

    # Memory Cleanup
    del generated_ids, full_input_ids, outputs, attentions, output_attn_data
    torch.cuda.empty_cache()
    gc.collect()

print("ì‘ì—… ì™„ë£Œ")