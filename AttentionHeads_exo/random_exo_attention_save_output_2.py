import os
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

import gc
import numpy as np
import pandas as pd
import torch
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from file_managing import make_input_image, make_input_image_exo
from config import AGD20K_PATH, model_name

# ------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì •
# ------------------------------------------------------
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_ENABLE_SDPA"] = "1"

print(f"ğŸ¤– {model_name} ëª¨ë¸ ë¡œë”©ì¤‘...")

# Qwen3 Model & Processor ë¡œë“œ
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager", # Attention Map ì¶”ì¶œì„ ìœ„í•´ eager ëª¨ë“œ í•„ìˆ˜
    device_map="cuda", 
)
processor = AutoProcessor.from_pretrained(model_name)
device = model.device

# ------------------------------------------------------
# 2. ì‹¤í–‰ë¶€
# ------------------------------------------------------
system_prompt = (
    "You are a helpful language and vision assistant. "
    "You are able to understand the visual content that the user provides, "
    "and assist the user with a variety of tasks using natural language."
)

df_fin = pd.read_pickle("target_df_w_random_exo.pkl")
# ê²°ê³¼ë¥¼ ë‹´ì„ ìƒˆë¡œìš´ ì»¬ëŸ¼ë“¤
df_fin['output_sentence'] = "" 
df_fin['output_attentions'] = "" # ì—¬ê¸°ì— ë³µì¡í•œ êµ¬ì¡°ì˜ ë°ì´í„°ê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.

print(f"length of Data : {len(df_fin)}")

for index, row in df_fin.iterrows():
    # if (index < 74) or (index > 80):
    #     continue
    object_name = row['object']
    action = row['action']
    filename = row['filename']
    
    
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    description = f"""Refer to the second image (exocentric view) for context. 
    Based on the first image (egocentric view), when people perform {action} with {object_name}, which part of the {object_name} is used for '{action}'?
    Answer in one sentence."""


    file_name_real = f"{AGD20K_PATH}/Seen/testset/egocentric/{action}/{object_name}/{filename}"
    exo_file_name_real = row['random_exo_filename'].replace("/home/DATA/AGD20K", AGD20K_PATH)
    print(f"\n{index} >>> {object_name} | {action} | {filename}")
    
    image_base64 = make_input_image_exo(file_name_real)
    exo_image_base64 = make_input_image_exo(exo_file_name_real)

    messages = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
        {"role": "user", "content": [
            {"type": "image", "image": f"data:image/jpeg;base64,{image_base64}"},
            {"type": "image", "image": f"data:image/jpeg;base64,{exo_image_base64}"},
            {"type": "text", "text": description}, 
        ]}
    ]

    # -------------------------------------------------------
    # STEP 1: Pre-process Inputs
    # -------------------------------------------------------
    inputs = processor.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True, # ë‹µë³€ ìƒì„±ì„ ìœ„í•´ Trueë¡œ ë³€ê²½
        return_dict=True,
        return_tensors="pt"
    )
    inputs = inputs.to(device)

    # -------------------------------------------------------
    # STEP 2: Generate Output Sentence (ë‹µë³€ ìƒì„±)
    # -------------------------------------------------------
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=1024, # í•„ìš”í•œ ë§Œí¼ ì¡°ì ˆ
            do_sample=False,    # Deterministic
            use_cache=True
        )
    
    # ìƒì„±ëœ ë‹µë³€ë§Œ ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
    input_len = inputs.input_ids.shape[1]
    output_ids = generated_ids[0][input_len:] # ìˆœìˆ˜ ìƒì„±ëœ í† í° IDë“¤
    output_text = processor.decode(output_ids, skip_special_tokens=True)
    
    print(f"ğŸ“ Output Sentence: {output_text}")
    df_fin.at[index, 'output_sentence'] = output_text

    # -------------------------------------------------------
    # STEP 3: Forward Pass with Full Sequence (Attention ì¶”ì¶œ)
    # -------------------------------------------------------
    # ìƒì„±ëœ ì „ì²´ ì‹œí€€ìŠ¤(Input + Output)ë¥¼ ë‹¤ì‹œ ëª¨ë¸ì— ë„£ì–´ Attention Mapì„ êµ¬í•¨
    # QwenVLì€ image inputs(pixel_values ë“±)ì´ í•„ìš”í•˜ë¯€ë¡œ inputs ì •ë³´ ì¬ì‚¬ìš©
    
    full_input_ids = generated_ids # [1, seq_len]
    
    # inputsì— ìˆëŠ” ì´ë¯¸ì§€ ê´€ë ¨ í…ì„œë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, input_idsë§Œ ì „ì²´ ì‹œí€€ìŠ¤ë¡œ êµì²´
    # ì£¼ì˜: Qwen3VLForConditionalGenerationì˜ forward ì¸ìì— ë§ì¶° ì „ë‹¬
    with torch.no_grad():
        outputs = model(
            input_ids=full_input_ids,
            pixel_values=inputs.pixel_values,
            image_grid_thw=inputs.image_grid_thw,
            attention_mask=torch.ones_like(full_input_ids), # ì „ì²´ ë‹¤ ë³´ì´ê²Œ
            output_attentions=True,
            return_dict=True
        )
    
    attentions = outputs.attentions # Tuple of (batch, num_heads, seq_len, seq_len)
    
    # -------------------------------------------------------
    # STEP 4: Index Parsing
    # -------------------------------------------------------
    ids_list = full_input_ids[0].tolist()
    tok = processor.tokenizer
    
    # Vision Token ID ì •ì˜
    vision_start_id = tok.convert_tokens_to_ids("<|vision_start|>")
    vision_end_id   = tok.convert_tokens_to_ids("<|vision_end|>")

    # 1. ì‹œí€€ìŠ¤ ë‚´ì˜ 'ëª¨ë“ ' ì´ë¯¸ì§€ ì‹œì‘/ë ìœ„ì¹˜ ì°¾ê¸°
    all_starts = [i for i, x in enumerate(ids_list) if x == vision_start_id]
    all_ends   = [i for i, x in enumerate(ids_list) if x == vision_end_id]

# [ìˆ˜ì •] ì´ë¯¸ì§€ 2ê°œê°€ ì •ìƒì ìœ¼ë¡œ ìˆëŠ”ì§€ í™•ì¸
    if len(all_starts) < 2:
        print("Error: Expected 2 images, but found fewer.")
        raise Exception("Image not found in sequence.")
    else:
        # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ (Egocentric) ë²”ìœ„
        idx1_start, idx1_end = all_starts[0], all_ends[0]
        # ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ (Exocentric) ë²”ìœ„
        idx2_start, idx2_end = all_starts[1], all_ends[1]

    # íˆíŠ¸ë§µ ìƒì„±ì„ ìœ„í•œ Target(Ego) ì„¤ì •
    target_start = idx1_start
    target_end = idx1_end

    # Grid Info (ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ê¸°ì¤€)
    grid_t, grid_h, grid_w = inputs.image_grid_thw[0].detach().cpu().numpy()
    llm_grid_h = grid_h // 2
    llm_grid_w = grid_w // 2
    query_start_idx = input_len
    query_end_idx = len(ids_list)  
    output_attn_data = []

    # ê° Output Tokenì— ëŒ€í•´ ìˆœíšŒ (Query Iteration)
    for q_idx in range(query_start_idx, query_end_idx):
        token_id = ids_list[q_idx]
        token_str = tok.decode([token_id])
        
        token_data = {
            "token_idx_in_seq": q_idx,
            "token_str": token_str,
            "token_id": token_id,
            "attentions": [] # ê° ë ˆì´ì–´/í—¤ë“œì˜ heatmap ì •ë³´
        }

        # ëª¨ë“  Layer ìˆœíšŒ
        for layer_idx, layer_attn in enumerate(attentions):
            # layer_attn shape: [batch, num_heads, seq_len, seq_len]
            # [0, :, q_idx, :] -> í˜„ì¬ í† í°(q_idx)ì´ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ë°”ë¼ë³´ëŠ” Attention
            
            heads_attn = layer_attn[0, :, q_idx, :] # [num_heads, seq_len]
            num_heads = heads_attn.shape[0]

            for head_idx in range(num_heads):
                this_head_attn = heads_attn[head_idx] # [seq_len]
                
                # [ìˆ˜ì •] 1ë²ˆ ì´ë¯¸ì§€ Attention í•©
                attn_img1 = this_head_attn[idx1_start + 1 : idx1_end]
                sum_img1 = float(attn_img1.sum().detach().cpu().item())
                # [ìˆ˜ì •] 2ë²ˆ ì´ë¯¸ì§€ Attention í•© (ì¡´ì¬í•  ê²½ìš°)
                sum_img2 = 0.0
                attn_img2 = this_head_attn[idx2_start + 1 : idx2_end]
                sum_img2 = float(attn_img2.sum().detach().cpu().item())
                # [ìµœì¢…] ë‘ ì´ë¯¸ì§€ì˜ Attention ì´í•©
                total_s_img_val = sum_img1 + sum_img2

                # Heatmap ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ float16 ë“±ìœ¼ë¡œ ë³€í™˜ ê³ ë ¤ ê°€ëŠ¥)
                heatmap_np = attn_img1.reshape(llm_grid_h, llm_grid_w).float().cpu().numpy()
                
                # í•„ìš”í•œ ì •ë³´ë§Œ ì €ì¥ (ì „ì²´ ë§µì„ ë‹¤ ì €ì¥í•˜ë©´ ìš©ëŸ‰ì´ ë§¤ìš° í½ë‹ˆë‹¤!)
                # ì—¬ê¸°ì„œëŠ” ìš”ì²­ëŒ€ë¡œ "V ê°’ì„ ëª¨ë‘ ì €ì¥" í•˜ë„ë¡ heatmapì„ ì €ì¥í•©ë‹ˆë‹¤.
                token_data["attentions"].append({
                    "layer": layer_idx,
                    "head": head_idx,
                    "s_img": total_s_img_val, # <--- ì—¬ê¸°ê°€ ìˆ˜ì •ë¨ (1+2 í•©)
                    "s_img_ego": sum_img1,    # (ì˜µì…˜) ë‚˜ì¤‘ì„ ìœ„í•´ ë¶„ë¦¬í•´ì„œ ì €ì¥í•´ë„ ì¢‹ìŒ
                    "s_img_exo": sum_img2,    # (ì˜µì…˜) 
                    "heatmap": heatmap_np 
                })
        
        output_attn_data.append(token_data)

    df_fin.at[index, 'output_attentions'] = output_attn_data
    
    # -------------------------------------------------------
    # STEP 6: Save & Cleanup
    # -------------------------------------------------------
    save_every = 5 # ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ë” ìì£¼ ì €ì¥ ê¶Œì¥
    if (index % save_every == 0) and (index > 1):
        save_index = int(index/save_every)
        print(f"âœ… Saving!!!=== at index={index}")
        df_fin.iloc[index-5:index].to_pickle(f"exo_attention_result_32B_2_{save_index}.pkl")
        print(f"âœ… Saved at index={index} // {save_index}")

    # Memory Cleanup
    del generated_ids, full_input_ids, outputs, attentions, output_attn_data
    torch.cuda.empty_cache()
    gc.collect()

print("ì‘ì—… ì™„ë£Œ")