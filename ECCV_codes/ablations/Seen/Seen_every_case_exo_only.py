#  nohup python -u seen_every_case.py >> 3B_seen_every_case.log 2>&1 & 
import os
import sys
import gc
import cv2
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm
from PIL import Image
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, CLIPSegProcessor, CLIPSegForImageSegmentation

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append("/home/bongo/porter_notebook/research/qwen3")
from file_managing import (
    make_input_image,
    calculate_metrics, 
    load_ground_truth ,
    prompt_dict_obj
)

# â˜… MetricsTracker ì„í¬íŠ¸
from VLM_model_dot_relative import MetricsTracker

from config import AGD20K_PATH, model_name

# --- [ì¶”ê°€] Exo ì´ë¯¸ì§€ ë¡œë“œ í•¨ìˆ˜ ---
def make_input_image_exo(path):
    # ê¸°ì¡´ make_input_imageì™€ ë™ì¼í•˜ê±°ë‚˜ exo ì „ìš© ì „ì²˜ë¦¬ê°€ í•„ìš”í•  ê²½ìš° ìˆ˜ì •
    return make_input_image(path)

# ------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”©
# ------------------------------------------------------
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_ENABLE_SDPA"] = "1"

TARGET_ROOT = f"{AGD20K_PATH}/Seen/testset/egocentric"
GT_ROOT = f"{AGD20K_PATH}/Seen/testset/GT"

# ì´ì „ ë‹¨ê³„ì—ì„œ ì €ì¥í•œ ìµœì ì˜ Exo ì´ë¯¸ì§€ DB ë¡œë“œ
EXO_DB_PATH = "selected_best_exo_images.pkl" 
if not os.path.exists(EXO_DB_PATH):
    raise FileNotFoundError(f"Exo DBë¥¼ ë¨¼ì € ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤: {EXO_DB_PATH}")
df_exo_db = pd.read_pickle(EXO_DB_PATH)


SAVE_FILENAME = "attention_result_seen_5exp.pkl"
VIS_DIR = "result_vis_5exp"
os.makedirs(VIS_DIR, exist_ok=True)

print(f"ğŸ¤– {model_name} (Qwen) ë¡œë”©ì¤‘...")
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager",
    device_map="cuda", 
)
processor = AutoProcessor.from_pretrained(model_name)
device = model.device

print(f"ğŸ‘ï¸ CLIPSeg (Object Maskìš©) ë¡œë”©ì¤‘...")
clipseg_processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
clipseg_model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined").to(device)

# ------------------------------------------------------
# 2. Metrics Trackers ì´ˆê¸°í™” (ì‹¤í—˜ë³„ ê°ì²´ ìƒì„±)
# ------------------------------------------------------
# 5ê°€ì§€ ì‹¤í—˜ì— ëŒ€í•œ íŠ¸ë˜ì»¤ ìƒì„±
# (Experiment ë¦¬ìŠ¤íŠ¸ ë° íŠ¸ë˜ì»¤ ì´ˆê¸°í™” ë¶€ë¶„ì€ ê¸°ì¡´ê³¼ ë™ì¼)
trackers = {}
# Context (Exo+Ego) ì‹¤í—˜: Exp1 ~ Exp6
for i in range(1, 7):
    trackers[f'Exp{i}'] = MetricsTracker(name=f"Exp{i}_Context")

# ------------------------------------------------------
# 3. Helper Functions
# ------------------------------------------------------
def min_max_normalize(map_data):
    m_min, m_max = map_data.min(), map_data.max()
    if m_max - m_min == 0: return map_data
    return (map_data - m_min) / (m_max - m_min)

def get_clipseg_mask(image_path, text_prompt, target_h, target_w):
    image = Image.open(image_path).convert("RGB")
    inputs = clipseg_processor(text=[text_prompt], images=[image], padding="max_length", return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = clipseg_model(**inputs)
        preds = torch.sigmoid(outputs.logits)[0]
    
    heatmap_small = cv2.resize(preds.cpu().numpy(), (target_w, target_h))
    binary_mask = (heatmap_small > 0.15).astype(np.float32)
    return heatmap_small, binary_mask

def check_heatmap_containment(heatmap_top, heatmap_obj, threshold=0.15, containment_ratio=0.8):
    if hasattr(heatmap_top, 'cpu'): heatmap_top = heatmap_top.detach().cpu().numpy()
    if hasattr(heatmap_obj, 'cpu'): heatmap_obj = heatmap_obj.detach().cpu().numpy()

    mask_top = heatmap_top > threshold
    mask_obj = heatmap_obj > threshold
    area_top = np.sum(mask_top)
    
    if area_top == 0: return False
    
    is_smaller = area_top < np.sum(mask_obj)
    intersection = np.logical_and(mask_top, mask_obj)
    is_inside = np.sum(intersection) >= (area_top * containment_ratio)

    return is_smaller and is_inside

def apply_post_processing(heatmap, refinement_heatmap=None, w=224, h=224):
    if refinement_heatmap is not None:
        if heatmap.max() > 0: heatmap /= heatmap.max()
        heatmap = heatmap * refinement_heatmap
        heatmap = np.power(heatmap, 0.75) 
    else:
        if heatmap.max() > 0: heatmap /= heatmap.max()
            
    heatmap_resized = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_LINEAR)
    sig = min(w, h) * 0.05
    k_val = int(sig * 3) * 2 + 1
    blur_map = cv2.GaussianBlur(heatmap_resized, (k_val, k_val), sig)
    return min_max_normalize(blur_map)


# ------------------------------------------------------
# 4. ë°ì´í„°ì…‹ ìŠ¤ìº”
# ------------------------------------------------------
print(f"ğŸ“‚ {TARGET_ROOT} ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘...")
data_list = []
for action in sorted(os.listdir(TARGET_ROOT)):
    action_path = os.path.join(TARGET_ROOT, action)
    if not os.path.isdir(action_path): continue
    for obj in sorted(os.listdir(action_path)):
        obj_path = os.path.join(action_path, obj)
        if not os.path.isdir(obj_path): continue
        for file in sorted(os.listdir(obj_path)):
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                data_list.append({
                    'action': action,
                    'object': obj,
                    'filename': file,
                    'full_path': os.path.join(obj_path, file),
                    'gt_path': os.path.join(GT_ROOT, action, obj, file.replace('.jpg', '.png'))
                })

df_fin = pd.DataFrame(data_list)

# ì‹¤í—˜ë³„ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ ì´ˆê¸°í™”
exp_names = ['Exp1_LastInput', 'Exp2_AvgOutput', 'Exp3_Top1_Raw', 'Exp4_Top1_Obj', 'Exp5_Top1_Adapt']
metrics_keys = ['KLD', 'SIM', 'NSS']
for exp in exp_names:
    for metric in metrics_keys:
        df_fin[f"{exp}_{metric}"] = None
df_fin['top_token_text'] = None 

print(f"âœ… ì´ {len(df_fin)}ê°œì˜ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

# ------------------------------------------------------
# 5. Main Loop
# ------------------------------------------------------
system_prompt = "You are a helpful language and vision assistant."

for index, row in tqdm(df_fin.iterrows(), total=len(df_fin), desc="Processing"):
    action = row['action']
    object_name = row['object']
    ego_path = row['full_path']
    gt_path = row['gt_path']  
    PLSP_name = prompt_dict_obj[action][row['object']]

    filename = row['filename']
    orig_img = Image.open(ego_path).convert("RGB")
    w, h = orig_img.size

    # --- [STEP 0] Exo Context ì´ë¯¸ì§€ ì°¾ê¸° ---
    exo_row = df_exo_db[(df_exo_db['action'] == action) & (df_exo_db['object'] == object_name)]
    if exo_row.empty:
        # DBì— ì—†ìœ¼ë©´ ê°€ì¥ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¼ë„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìŠ¤í‚µ
        continue
    exo_path = exo_row.iloc[0]['best_exo_path']    
    
    # -------------------------------------------------------------------------
    description = f"Refer to the second image (exocentric view) for context. Based on the first image (egocentric view), when people perform {action} with {object_name.replace('_',' ')}, which part of the {object_name.replace('_',' ')} is used for '{action}'? Answer in one sentence."

    ego_base64 = make_input_image(ego_path)
    exo_base64 = make_input_image_exo(exo_path)

    messages = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
        {"role": "user", "content": [
            {"type": "image", "image": f"data:image/jpeg;base64,{ego_base64}"}, # Ego (First)
            {"type": "image", "image": f"data:image/jpeg;base64,{exo_base64}"}, # Exo (Second)
            {"type": "text", "text": description}
        ]}
    ]
    
    inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors="pt").to(device)

    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=50, do_sample=False)
        # ì „ì²´ ì–´í…ì…˜ ì¶”ì¶œì„ ìœ„í•œ ì¬ì¸í¼ëŸ°ìŠ¤ (use_cache=False ê¶Œì¥)
        outputs = model(input_ids=generated_ids, pixel_values=inputs.pixel_values, 
                        image_grid_thw=inputs.image_grid_thw, attention_mask=torch.ones_like(generated_ids), 
                        output_attentions=True, return_dict=True)


    attentions = outputs.attentions
    
    ids_list = generated_ids[0].tolist()

    # --- [STEP 2] Vision Token Range êµ¬ë¶„ (Ego vs Exo) ---
    v_start_token = processor.tokenizer.convert_tokens_to_ids("<|vision_start|>")
    v_end_token = processor.tokenizer.convert_tokens_to_ids("<|vision_end|>")
    
    # ëª¨ë“  vision_start ì¸ë±ìŠ¤ ì°¾ê¸°
    v_starts = [i for i, x in enumerate(ids_list) if x == v_start_token]
    v_ends = [i for i, x in enumerate(ids_list) if x == v_end_token]
    
    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ê°€ Ego ì´ë¯¸ì§€ì„
    ego_vis_start = v_starts[0]
    ego_vis_end = v_ends[0]
    
    # ê·¸ë¦¬ë“œ ì •ë³´ (Ego ê¸°ì¤€)
    grid_t, grid_h, grid_w = inputs.image_grid_thw[0].detach().cpu().numpy()
    llm_grid_h, llm_grid_w = grid_h // 2, grid_w // 2
    # -------------------------------------------------------------------------
    # [PRE-CALCULATION] Prepare Base Maps
    # -------------------------------------------------------------------------
    clip_object_heatmap, clip_object_mask = get_clipseg_mask(ego_path, object_name.replace('_',' '), llm_grid_h, llm_grid_w)
    clip_plsp_heatmap, clip_plsp_mask = get_clipseg_mask(ego_path, PLSP_name.replace('_',' '), llm_grid_h, llm_grid_w)

    # -------------------------------------------------------------------------
    # [STEP 3] Attention Extraction (Last Input & Top-1 Selection)
    # -------------------------------------------------------------------------
    input_len = inputs.input_ids.shape[1]
    token_candidates = []
    
    # Ego ì´ë¯¸ì§€ìš© CLIPSeg Mask (Top-1 ìŠ¤ì½”ì–´ë§ ë° Refinementìš©)
    clip_obj_heatmap, clip_obj_mask = get_clipseg_mask(ego_path, object_name.replace('_',' '), llm_grid_h, llm_grid_w)

    # --- (1) Exp1: Last Input Token Attention (Ego-Targeted) ---
    # ì§ˆë¬¸ ì…ë ¥ì´ ëë‚œ ì§í›„(last_input_idx)ì˜ ì–´í…ì…˜ ìƒíƒœ
    last_input_idx = input_len - 1
    map_last_input_torch = torch.zeros((llm_grid_h * llm_grid_w), device=device)
    
    for layer_attn in attentions:
        # q_idx = last_input_idx, key = ego_vis ì˜ì—­ë§Œ ìŠ¬ë¼ì´ì‹±
        heads_attn_last = layer_attn[0, :, last_input_idx, ego_vis_start+1 : ego_vis_end]
        map_last_input_torch += heads_attn_last.sum(dim=0)
    
    np_map_last_input = map_last_input_torch.reshape(llm_grid_h, llm_grid_w).cpu().numpy().astype(np.float32)

    # --- (2) Exp2 & Top-1: Output Tokens Attention (Ego-Targeted) ---
    map_avg_accum = torch.zeros((llm_grid_h * llm_grid_w), device=device)
    token_count = 0

    for q_idx in range(input_len, len(ids_list)):
        step_heatmap = torch.zeros((llm_grid_h * llm_grid_w), device=device)
        for layer_attn in attentions:
            # ëª¨ë“  ìƒì„± í† í°ì— ëŒ€í•´ ì˜¤ì§ Ego ì´ë¯¸ì§€ ì˜ì—­ë§Œ ê´€ì°°
            heads_attn_step = layer_attn[0, :, q_idx, ego_vis_start+1 : ego_vis_end]
            step_heatmap += heads_attn_step.sum(dim=0)
        
        # Avg Outputìš© ëˆ„ì 
        map_avg_accum += step_heatmap
        token_count += 1
        
        # Top-1 í›„ë³´ì§€ ì„ ì •ì„ ìœ„í•œ ìŠ¤ì½”ì–´ë§ (Ego Object Mask í™œìš©)
        heatmap_np = step_heatmap.reshape(llm_grid_h, llm_grid_w).cpu().numpy().astype(np.float32)
        score = (heatmap_np * clip_obj_mask).sum()
        
        token_candidates.append({
            "idx": q_idx, 
            "str": processor.tokenizer.decode([ids_list[q_idx]]),
            "score": score, 
            "heatmap": heatmap_np
        })

    # Avg Output Map ìµœì¢… ê³„ì‚°
    if token_count > 0:
        map_avg_output = (map_avg_accum / token_count).reshape(llm_grid_h, llm_grid_w).cpu().numpy().astype(np.float32)
    else:
        map_avg_output = np.zeros((llm_grid_h, llm_grid_w), dtype=np.float32)

    # Top-1 í† í° ê²°ì • (Ego ì´ë¯¸ì§€ì— ê°€ì¥ ë†’ì€ Attention ì—ë„ˆì§€ë¥¼ ìŸì€ í† í°)
    if token_candidates:
        # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ í† í° ì •ë ¬
        sorted_candidates = sorted(token_candidates, key=lambda x: x['score'], reverse=True)
        top_token = sorted_candidates[0]
        
        map_top1 = top_token['heatmap']
        top_token_text = top_token['str']
        
        # Exp5ë¥¼ ìœ„í•œ following_text ì¶”ì¶œ (ë¬¸ë§¥ íŒŒì•…ìš©)
        next_idx = top_token['idx'] + 1
        following_text = ""
        for cand in token_candidates:
            if cand['idx'] == next_idx:
                following_text = cand['str']
                break
    else:
        map_top1 = np.zeros((llm_grid_h, llm_grid_w), dtype=np.float32)
        top_token_text = "None"
        following_text = ""

    # -------------------------------------------------------------------------
    # [EXPERIMENTS] Generate Final Maps
    # -------------------------------------------------------------------------
    final_maps = {} 
    final_maps['Exp1'] = apply_post_processing(np_map_last_input.copy(), refinement_heatmap=None, w=w, h=h)
    final_maps['Exp2'] = apply_post_processing(map_avg_output.copy(), refinement_heatmap=None, w=w, h=h)
    final_maps['Exp3'] = apply_post_processing(map_top1.copy(), refinement_heatmap=None, w=w, h=h)
    final_maps['Exp4'] = apply_post_processing(map_top1.copy(), refinement_heatmap=clip_object_heatmap, w=w, h=h)
    
    refined_prompt = f"{top_token_text} {following_text}".replace('.', '').strip()
    clip_specific_heatmap, clip_specific_mask = get_clipseg_mask(ego_path, refined_prompt, llm_grid_h, llm_grid_w)
    
    if check_heatmap_containment(clip_specific_mask, clip_object_mask):
        adaptive_heatmap = clip_specific_heatmap 
        exp5_label = f"Part('{refined_prompt}')"
    else:
        adaptive_heatmap = clip_object_heatmap
        exp5_label = f"Obj('{object_name}')"
    final_maps['Exp5'] = apply_post_processing(map_top1.copy(), refinement_heatmap=adaptive_heatmap, w=w, h=h)
    final_maps['Exp6'] = apply_post_processing(map_top1.copy(), refinement_heatmap=clip_plsp_heatmap, w=w, h=h)

    # -------------------------------------------------------------------------
    # [EVALUATION] Calculate Metrics & UPDATE TRACKERS
    # -------------------------------------------------------------------------
    gt_map = load_ground_truth(gt_path) 
    
    if gt_map is not None:
        if gt_map.shape != (h, w):
                gt_map = cv2.resize(gt_map, (w, h), interpolation=cv2.INTER_NEAREST)

        # ì‹¤í—˜ í‚¤ ë§¤í•‘ (ë‚´ë¶€ í‚¤ -> ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ í”„ë¦¬í”½ìŠ¤)
        key_map = {
            'Exp1': 'Exp1_LastInput',
            'Exp2': 'Exp2_AvgOutput',
            'Exp3': 'Exp3_Top1_Raw',
            'Exp4': 'Exp4_Top1_Obj',
            'Exp5': 'Exp5_Top1_Adapt',
            'Exp6': 'Exp6_PLSP'
        }

        print(f"\n--- Metrics Update [{index}] ---")
        for exp_key, pred_map in final_maps.items():
            col_prefix = key_map[exp_key]
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = calculate_metrics(pred_map, gt_map)
            
            # 1. ë°ì´í„°í”„ë ˆì„ ì €ì¥
            df_fin.at[index, f"{col_prefix}_KLD"] = metrics['KLD']
            df_fin.at[index, f"{col_prefix}_SIM"] = metrics['SIM']
            df_fin.at[index, f"{col_prefix}_NSS"] = metrics['NSS']

            # 2. â˜… Tracker ì—…ë°ì´íŠ¸ ë° ì¶œë ¥
            tracker = trackers[exp_key]
            tracker.update(metrics)
            tracker.print_metrics(metrics, filename)

    df_fin.at[index, 'top_token_text'] = top_token_text

    # -------------------------------------------------------------------------
    # [VISUALIZATION] Save Comparison Plot
    # -------------------------------------------------------------------------
    fig, axes = plt.subplots(1, 8, figsize=(28, 4))
    axes[0].imshow(orig_img); axes[0].set_title(f"Original\n{object_name}"); axes[0].axis('off')
    
    if gt_map is not None:
        axes[1].imshow(gt_map, cmap='gray'); axes[1].set_title("GT")
    else:
        axes[1].set_title("No GT")
    axes[1].axis('off')
    
    exp_titles = ["Exp1 LastIn", "Exp2 AvgOut", "Exp3 Top1", "Exp4 Top1+Obj", f"Exp5 {exp5_label}", "Exp6 PLSP"]
    for i, (key, title) in enumerate(zip(['Exp1', 'Exp2', 'Exp3', 'Exp4', 'Exp5', 'Exp6'], exp_titles)):
        ax = axes[i+2]
        ax.imshow(final_maps[key], cmap='jet', vmin=0, vmax=1)
        ax.set_title(title, fontsize=10); ax.axis('off')
        
        # ë©”íŠ¸ë¦­ í‘œì‹œ
        if gt_map is not None:
            tracker = trackers[key] # í˜„ì¬ íŠ¸ë˜ì»¤ì˜ í‰ê· ê°’ ê°€ì ¸ì˜¤ê¸° ê°€ëŠ¥
            sim = df_fin.at[index, f"{key_map[key]}_SIM"]
            nss = df_fin.at[index, f"{key_map[key]}_NSS"]
            ax.text(0.5, -0.1, f"S:{sim:.2f} N:{nss:.2f}", transform=ax.transAxes, ha='center', fontsize=9, color='blue')

    save_path = os.path.join(VIS_DIR, f"{action}_{object_name}_{filename.split('.')[0]}.png")
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close(fig)

    # -------------------------------------------------------------------------
    # Memory Cleanup
    # -------------------------------------------------------------------------
    if index % 50 == 0:
        df_fin.to_pickle(SAVE_FILENAME)
    
    del generated_ids, outputs, attentions, token_candidates, final_maps
    torch.cuda.empty_cache()
    gc.collect()



df_fin.to_pickle(SAVE_FILENAME)
print(f"ğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ! ê²°ê³¼ ì €ì¥: {SAVE_FILENAME}")

# ìµœì¢… í‰ê·  ì¶œë ¥
print("\nğŸ“Š Final Average Metrics:")
for key, tracker in trackers.items():
    print(f"[{key}] KLD: {tracker.KLD.avg:.4f} | SIM: {tracker.SIM.avg:.4f} | NSS: {tracker.NSS.avg:.4f}")