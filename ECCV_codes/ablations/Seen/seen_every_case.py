#  nohup python -u seen_every_case.py >> 2B_seen_some_case.log 2>&1 & 
#  nohup python -u seen_every_case.py >> 2B_seen_all_case.log 2>&1 & 
import os
import sys
import gc
import cv2
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from tqdm import tqdm
from PIL import Image
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, CLIPSegProcessor, CLIPSegForImageSegmentation

# ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append("/home/bongo/porter_notebook/research/qwen3")
from file_managing import (
    make_input_image,
    calculate_metrics, 
    load_ground_truth ,
    prompt_dict_obj
)

# â˜… MetricsTracker ì„í¬íŠ¸
from VLM_model_dot_relative import MetricsTracker

from config import AGD20K_PATH, model_name

# --- [ì¶”ê°€] Exo ì´ë¯¸ì§€ ë¡œë“œ í•¨ìˆ˜ ---
def make_input_image_exo(path):
    # ê¸°ì¡´ make_input_imageì™€ ë™ì¼í•˜ê±°ë‚˜ exo ì „ìš© ì „ì²˜ë¦¬ê°€ í•„ìš”í•  ê²½ìš° ìˆ˜ì •
    return make_input_image(path)

# ------------------------------------------------------
# 1. í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”©
# ------------------------------------------------------
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_ENABLE_SDPA"] = "1"

TARGET_ROOT = f"{AGD20K_PATH}/Seen/testset/egocentric"
GT_ROOT = f"{AGD20K_PATH}/Seen/testset/GT"

# ì´ì „ ë‹¨ê³„ì—ì„œ ì €ì¥í•œ ìµœì ì˜ Exo ì´ë¯¸ì§€ DB ë¡œë“œ
EXO_DB_PATH = "selected_best_exo_images.pkl" 
if not os.path.exists(EXO_DB_PATH):
    raise FileNotFoundError(f"Exo DBë¥¼ ë¨¼ì € ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤: {EXO_DB_PATH}")
df_exo_db = pd.read_pickle(EXO_DB_PATH)


SAVE_FILENAME = "2B_attention_result_seen_all.pkl"
VIS_DIR = "2B_result_vis_all"
os.makedirs(VIS_DIR, exist_ok=True)

print(f"ğŸ¤– {model_name} (Qwen) ë¡œë”©ì¤‘...")
model = Qwen3VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    attn_implementation="eager",
    device_map="cuda", 
)
processor = AutoProcessor.from_pretrained(model_name)
device = model.device

print(f"ğŸ‘ï¸ CLIPSeg (Object Maskìš©) ë¡œë”©ì¤‘...")
clipseg_processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
clipseg_model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined").to(device)

# ------------------------------------------------------
# 2. Metrics Trackers ì´ˆê¸°í™” (ì‹¤í—˜ë³„ ê°ì²´ ìƒì„±)
# ------------------------------------------------------
# 5ê°€ì§€ ì‹¤í—˜ì— ëŒ€í•œ íŠ¸ë˜ì»¤ ìƒì„±
# (Experiment ë¦¬ìŠ¤íŠ¸ ë° íŠ¸ë˜ì»¤ ì´ˆê¸°í™” ë¶€ë¶„ì€ ê¸°ì¡´ê³¼ ë™ì¼)
trackers = {}
# Context (Exo+Ego) ì‹¤í—˜: Exp1 ~ Exp6
for i in range(1, 7):
    trackers[f'Exp{i}'] = MetricsTracker(name=f"Exp{i}_Context")
# Baseline (Ego Only) ì‹¤í—˜: Exp7 ~ Exp12
for i in range(7, 13):
    trackers[f'Exp{i}'] = MetricsTracker(name=f"Exp{i}_EgoOnly")
# ------------------------------------------------------
# 3. Helper Functions
# ------------------------------------------------------
def min_max_normalize(map_data):
    m_min, m_max = map_data.min(), map_data.max()
    if m_max - m_min == 0: return map_data
    return (map_data - m_min) / (m_max - m_min)

def get_clipseg_mask(image_path, text_prompt, target_h, target_w):
    image = Image.open(image_path).convert("RGB")
    inputs = clipseg_processor(text=[text_prompt], images=[image], padding="max_length", return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = clipseg_model(**inputs)
        preds = torch.sigmoid(outputs.logits)[0]
    
    heatmap_small = cv2.resize(preds.cpu().numpy(), (target_w, target_h))
    binary_mask = (heatmap_small > 0.15).astype(np.float32)
    return heatmap_small, binary_mask

def check_heatmap_containment(heatmap_top, heatmap_obj, threshold=0.15, containment_ratio=0.8):
    if hasattr(heatmap_top, 'cpu'): heatmap_top = heatmap_top.detach().cpu().numpy()
    if hasattr(heatmap_obj, 'cpu'): heatmap_obj = heatmap_obj.detach().cpu().numpy()

    mask_top = heatmap_top > threshold
    mask_obj = heatmap_obj > threshold
    area_top = np.sum(mask_top)
    
    if area_top == 0: return False
    
    is_smaller = area_top < np.sum(mask_obj)
    intersection = np.logical_and(mask_top, mask_obj)
    is_inside = np.sum(intersection) >= (area_top * containment_ratio)

    return is_smaller and is_inside

def apply_post_processing(heatmap, refinement_heatmap=None, w=224, h=224):
    if refinement_heatmap is not None:
        if heatmap.max() > 0: heatmap /= heatmap.max()
        heatmap = heatmap * refinement_heatmap
        heatmap = np.power(heatmap, 0.75) 
    else:
        if heatmap.max() > 0: heatmap /= heatmap.max()
            
    heatmap_resized = cv2.resize(heatmap, (w, h), interpolation=cv2.INTER_LINEAR)
    sig = min(w, h) * 0.05
    k_val = int(sig * 3) * 2 + 1
    blur_map = cv2.GaussianBlur(heatmap_resized, (k_val, k_val), sig)
    return min_max_normalize(blur_map)

def extract_all_maps(attentions, ids_list, input_len, vis_start, vis_end, ego_path, object_name, plsp_name,llm_grid_h, llm_grid_w ):
    """
    attentions: ëª¨ë¸ì—ì„œ ì¶œë ¥ëœ attention íŠœí”Œ
    ids_list: generated_ids ë¦¬ìŠ¤íŠ¸
    input_len: prompt ì…ë ¥ ê¸¸ì´ (output ì‹œì‘ì )
    vis_start, vis_end: Grounding ëŒ€ìƒì´ ë˜ëŠ” ì´ë¯¸ì§€(Ego)ì˜ í† í° ë²”ìœ„
    """
    # 0. ì´ˆê¸°í™”
    device = attentions[0].device
    grid_h, grid_w = llm_grid_h, llm_grid_w  # ì „ì—­ ë³€ìˆ˜ í˜¹ì€ ì¸ìë¡œ ì „ë‹¬
    final_maps = {}
    
    # Ego ì´ë¯¸ì§€ ì •ë³´ (Post-processingìš©)
    orig_img = Image.open(ego_path).convert("RGB")
    w, h = orig_img.size

    # -------------------------------------------------------------------------
    # 1. Base Attention ì¶”ì¶œ (Last Input, Avg Output, Top-1)
    # -------------------------------------------------------------------------
    
    # (1) Exp1: Last Input (ì§ˆë¬¸ ì§í›„ Egoë¥¼ ë³´ëŠ” ëˆˆ)
    last_input_idx = input_len - 1
    map_last_input = torch.zeros((grid_h * grid_w), device=device)
    for layer_attn in attentions:
        # last_input_idx í† í°ì´ vis_start:vis_end ì˜ì—­ì„ ë³´ëŠ” ì–´í…ì…˜
        heads_attn = layer_attn[0, :, last_input_idx, vis_start+1 : vis_end]
        map_last_input += heads_attn.sum(dim=0)
    np_map_last_input = map_last_input.reshape(grid_h, grid_w).cpu().numpy().astype(np.float32)

    # (2) Exp2 & Top-1 ìŠ¤ì½”ì–´ë§
    map_avg_accum = torch.zeros((grid_h * grid_w), device=device)
    token_candidates = []
    token_count = 0
    
    # Ego Object Mask (Top-1 ì„ ì •ì„ ìœ„í•œ í•„í„°)
    clip_obj_heatmap, clip_obj_mask = get_clipseg_mask(ego_path, object_name.replace('_', ' '), grid_h, grid_w)

    for q_idx in range(input_len, len(ids_list)):
        step_heatmap = torch.zeros((grid_h * grid_w), device=device)
        for layer_attn in attentions:
            heads_attn = layer_attn[0, :, q_idx, vis_start+1 : vis_end]
            step_heatmap += heads_attn.sum(dim=0)
        
        map_avg_accum += step_heatmap
        token_count += 1
        
        heatmap_np = step_heatmap.reshape(grid_h, grid_w).cpu().numpy().astype(np.float32)
        # í•´ë‹¹ í† í°ì´ Ego Object ì˜ì—­ì„ ì–¼ë§ˆë‚˜ ì„¤ëª…í•˜ëŠ”ì§€ ì ìˆ˜í™”
        score = (heatmap_np * clip_obj_mask).sum()
        
        token_candidates.append({
            "idx": q_idx,
            "str": processor.tokenizer.decode([ids_list[q_idx]]),
            "score": score,
            "heatmap": heatmap_np
        })

    # Exp2: Avg Output
    if token_count > 0:
        map_avg_output = (map_avg_accum / token_count).reshape(grid_h, grid_w).cpu().numpy().astype(np.float32)
    else:
        map_avg_output = np.zeros((grid_h, grid_w), dtype=np.float32)

    # Exp3: Top-1 (Ego-best token)
    if token_candidates:
        sorted_cand = sorted(token_candidates, key=lambda x: x['score'], reverse=True)
        top_token = sorted_cand[0]
        map_top1 = top_token['heatmap']
        top_token_text = top_token['str']
        
        # Following text (for Exp5 context)
        next_idx = top_token['idx'] + 1
        following_text = next((c['str'] for c in token_candidates if c['idx'] == next_idx), "")
    else:
        map_top1 = np.zeros((grid_h, grid_w), dtype=np.float32)
        top_token_text, following_text = "none", ""

    # -------------------------------------------------------------------------
    # 2. ê°€ê³µ ë° Refinement (Exp 1 ~ 6)
    # -------------------------------------------------------------------------
    
    # Exp 1, 2, 3 (Raw Attention Maps)
    final_maps['Exp1'] = apply_post_processing(np_map_last_input.copy(), w=w, h=h)
    final_maps['Exp2'] = apply_post_processing(map_avg_output.copy(), w=w, h=h)
    final_maps['Exp3'] = apply_post_processing(map_top1.copy(), w=w, h=h)
    
    # Exp 4: Top-1 + Ego Object Prior
    final_maps['Exp4'] = apply_post_processing(map_top1.copy(), refinement_heatmap=clip_obj_heatmap, w=w, h=h)
    
    # Exp 5: Top-1 + Adaptive Part/Obj Prior
    refined_prompt = f"{top_token_text} {following_text}".replace('.', '').strip()
    clip_spec_heatmap, clip_spec_mask = get_clipseg_mask(ego_path, refined_prompt, grid_h, grid_w)
    
    if check_heatmap_containment(clip_spec_mask, clip_obj_mask):
        adaptive_refine = clip_spec_heatmap # Partë¡œ ì¸ì‹ë¨
    else:
        adaptive_refine = clip_obj_heatmap # Objë¡œ Fallback
    final_maps['Exp5'] = apply_post_processing(map_top1.copy(), refinement_heatmap=adaptive_refine, w=w, h=h)
    
    # Exp 6: Top-1 + PLSP (Semantic Prior)
    clip_plsp_heatmap, _ = get_clipseg_mask(ego_path, plsp_name.replace('_', ' '), grid_h, grid_w)
    final_maps['Exp6'] = apply_post_processing(map_top1.copy(), refinement_heatmap=clip_plsp_heatmap, w=w, h=h)

    return final_maps, refined_prompt
    
# ------------------------------------------------------
# 4. ë°ì´í„°ì…‹ ìŠ¤ìº”
# ------------------------------------------------------
print(f"ğŸ“‚ {TARGET_ROOT} ë””ë ‰í† ë¦¬ ìŠ¤ìº” ì¤‘...")
data_list = []
for action in sorted(os.listdir(TARGET_ROOT)):
    action_path = os.path.join(TARGET_ROOT, action)
    if not os.path.isdir(action_path): continue
    for obj in sorted(os.listdir(action_path)):
        obj_path = os.path.join(action_path, obj)
        if not os.path.isdir(obj_path): continue
        for file in sorted(os.listdir(obj_path)):
            if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                data_list.append({
                    'action': action,
                    'object': obj,
                    'filename': file,
                    'full_path': os.path.join(obj_path, file),
                    'gt_path': os.path.join(GT_ROOT, action, obj, file.replace('.jpg', '.png'))
                })

df_raw = pd.DataFrame(data_list)

# --- [ì¶”ê°€] Action-Object í˜ì–´ë³„ë¡œ 1ê°œë§Œ ìƒ˜í”Œë§ ---
# ê° ê·¸ë£¹(action, object)ì—ì„œ ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ì„ íƒ
# df_fin = df_raw.groupby(['action', 'object']).first().reset_index()

# ë§Œì•½ ë¬´ì‘ìœ„ë¡œ 1ê°œë¥¼ ë½‘ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:
# df_fin = df_raw.groupby(['action', 'object']).sample(n=1, random_state=42).reset_index()
df_fin = df_raw.copy()
# ì‹¤í—˜ë³„ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ ì´ˆê¸°í™”
exp_names = [
    'Exp1_Context_LastInput', 'Exp2_Context_AvgOutput', 'Exp3_Context_Top1_Raw', 
    'Exp4_Context_Top1_Obj', 'Exp5_Context_Top1_Adapt', 'Exp6_Context_PLSP',
    'Exp7_EgoOnly_LastInput', 'Exp8_EgoOnly_AvgOutput', 'Exp9_EgoOnly_Top1_Raw',
    'Exp10_EgoOnly_Top1_Obj', 'Exp11_EgoOnly_Top1_Adapt', 'Exp12_EgoOnly_PLSP'
]
metrics_keys = ['KLD', 'SIM', 'NSS']
for exp in exp_names:
    for metric in metrics_keys:
        df_fin[f"{exp}_{metric}"] = None

df_fin['top_token_text'] = None 
df_fin['exo_filename'] = None # Exo íŒŒì¼ëª… ì¶”ì ìš©

print(f"âœ… ì´ {len(df_fin)}ê°œì˜ Action-Object í˜ì–´ ì¤€ë¹„ ì™„ë£Œ (í˜ì–´ë‹¹ 1ê°œ ìƒ˜í”Œë§).")

# ------------------------------------------------------
# 5. Main Loop
# ------------------------------------------------------
system_prompt = "You are a helpful language and vision assistant."

for index, row in tqdm(df_fin.iterrows(), total=len(df_fin), desc="Processing"):
    action = row['action']
    object_name = row['object']
    ego_path = row['full_path']
    gt_path = row['gt_path']  
    PLSP_name = prompt_dict_obj[action][row['object']]

    filename = row['filename']
    orig_img = Image.open(ego_path).convert("RGB")
    w, h = orig_img.size

    # --- [STEP 0] Exo Context ì´ë¯¸ì§€ ì°¾ê¸° ---
    exo_row = df_exo_db[(df_exo_db['action'] == action) & (df_exo_db['object'] == object_name)]
    if exo_row.empty:
        # DBì— ì—†ìœ¼ë©´ ê°€ì¥ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¼ë„ ê°€ì ¸ì˜¤ê±°ë‚˜ ìŠ¤í‚µ
        continue
    exo_path = exo_row.iloc[0]['best_exo_path']    
    v_start_token = processor.tokenizer.convert_tokens_to_ids("<|vision_start|>")
    v_end_token = processor.tokenizer.convert_tokens_to_ids("<|vision_end|>")

    # -------------------------------------------------------------------------
    # PART A: Context-Aware Inference (Ego + Exo) -> Exp 1-6
    # -------------------------------------------------------------------------
    desc_context = f"Refer to the second image (exocentric view) for context. Based on the first image (egocentric view), when people perform {action} with {object_name.replace('_',' ')}, which part of the {object_name.replace('_',' ')} is used for '{action}'? Answer in one sentence."
    
    ego_b64 = make_input_image(ego_path)
    exo_b64 = make_input_image_exo(exo_path)

    msg_context = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
        {"role": "user", "content": [
            {"type": "image", "image": f"data:image/jpeg;base64,{ego_b64}"}, # Ego
            {"type": "image", "image": f"data:image/jpeg;base64,{exo_b64}"}, # Exo
            {"type": "text", "text": desc_context}
        ]}
    ]
    
    # --- [STEP A-1] Inference & Attention Extraction ---
    in_ctx = processor.apply_chat_template(msg_context, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors="pt").to(device)
    with torch.no_grad():
        gen_ctx = model.generate(**in_ctx, max_new_tokens=1024, do_sample=False)
        out_ctx = model(input_ids=gen_ctx, pixel_values=in_ctx.pixel_values, image_grid_thw=in_ctx.image_grid_thw, attention_mask=torch.ones_like(gen_ctx), output_attentions=True, return_dict=True)
    
    # ì²« ë²ˆì§¸ ì´ë¯¸ì§€(Ego) ì˜ì—­ ì¸ë±ìŠ¤ ì°¾ê¸°
    ids_ctx = gen_ctx[0].tolist()
    v_starts = [i for i, x in enumerate(ids_ctx) if x == v_start_token]; v_ends = [i for i, x in enumerate(ids_ctx) if x == v_end_token]
    ego_start, ego_end = v_starts[0], v_ends[0] # ì²« ë²ˆì§¸ê°€ Ego

    grid_t, grid_h, grid_w = in_ctx.image_grid_thw[0].detach().cpu().numpy()
    llm_grid_h, llm_grid_w = grid_h // 2, grid_w // 2
    # ì–´í…ì…˜ ê³„ì‚° í•¨ìˆ˜ í˜¸ì¶œ (Part Aìš©)
    maps_context, adaptive_refine_exo = extract_all_maps(out_ctx.attentions, ids_ctx, in_ctx.input_ids.shape[1], ego_start, ego_end, ego_path, object_name, PLSP_name,llm_grid_h, llm_grid_w )

    # -------------------------------------------------------------------------
    # PART B: Ego-Only Inference (Ego) -> Exp 7-12
    # -------------------------------------------------------------------------
    desc_ego = f"When people perform {action} with {object_name.replace('_',' ')}, which part of the {object_name.replace('_',' ')} is used for '{action}'? Answer in one sentence."
    
    msg_ego = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
        {"role": "user", "content": [
            {"type": "image", "image": f"data:image/jpeg;base64,{ego_b64}"},
            {"type": "text", "text": desc_ego}
        ]}
    ]
    
    # --- [STEP B-1] Inference & Attention Extraction ---
    in_ego = processor.apply_chat_template(msg_ego, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors="pt").to(device)
    with torch.no_grad():
        gen_ego = model.generate(**in_ego, max_new_tokens=1024, do_sample=False)
        out_ego = model(input_ids=gen_ego, pixel_values=in_ego.pixel_values, image_grid_thw=in_ego.image_grid_thw, attention_mask=torch.ones_like(gen_ego), output_attentions=True, return_dict=True)
    
    # Ego ì´ë¯¸ì§€ ì˜ì—­ ì¸ë±ìŠ¤ (ì´ë¯¸ì§€ê°€ í•˜ë‚˜ë¿ì´ë¯€ë¡œ index 0 ì‚¬ìš©)
    ids_ego = gen_ego[0].tolist()
    v_starts_e = [i for i, x in enumerate(ids_ego) if x == v_start_token]; v_ends_e = [i for i, x in enumerate(ids_ego) if x == v_end_token]
    ego_only_start, ego_only_end = v_starts_e[0], v_ends_e[0]
    
    # ì–´í…ì…˜ ê³„ì‚° í•¨ìˆ˜ í˜¸ì¶œ (Part Bìš©)
    maps_ego_only, adaptive_refine_ego = extract_all_maps(out_ego.attentions, ids_ego, in_ego.input_ids.shape[1], ego_only_start, ego_only_end, ego_path, object_name, PLSP_name, llm_grid_h, llm_grid_w)



    # -------------------------------------------------------------------------
    # [EVALUATION] GT ë¹„êµ ë° 12ê°œ íŠ¸ë˜ì»¤ ì—…ë°ì´íŠ¸
    # -------------------------------------------------------------------------
    gt_map = load_ground_truth(gt_path)
    
    if gt_map is not None:
        if gt_map.shape != (h, w):
            gt_map = cv2.resize(gt_map, (w, h), interpolation=cv2.INTER_NEAREST)

        # ì‹¤í—˜ ë§¤í•‘ ì •ì˜
        # trackers['Exp1'] ~ ['Exp6'] : Context (Ego+Exo)
        # trackers['Exp7'] ~ ['Exp12'] : Ego-Only (Baseline)
        
        print(f"\n--- Metrics Update [{index}] ---")
        
        # 1. Context Experiments (1-6)
        for i in range(1, 7):
            exp_key = f'Exp{i}'
            pred_map = maps_context[exp_key]
            metrics = calculate_metrics(pred_map, gt_map)
            
            # Tracker & DataFrame ì—…ë°ì´íŠ¸
            trackers[exp_key].update(metrics)
            for m_key in ['KLD', 'SIM', 'NSS']:
                df_fin.at[index, f"{exp_key}_Context_{m_key}"] = metrics[m_key]
            
            # ë””ë²„ê·¸ ì¶œë ¥ (Exp3ë§Œ ëŒ€í‘œë¡œ)
            trackers[exp_key].print_metrics(metrics, f"[Ctx] {filename}")

        # 2. Ego-Only Experiments (7-12)
        for i in range(7, 13):
            exp_key = f'Exp{i}'
            # extract_all_mapsëŠ” ë‚´ë¶€ì ìœ¼ë¡œ Exp1~6 í‚¤ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ i-6ìœ¼ë¡œ ì ‘ê·¼
            pred_map = maps_ego_only[f'Exp{i-6}'] 
            metrics = calculate_metrics(pred_map, gt_map)
            
            # Tracker & DataFrame ì—…ë°ì´íŠ¸
            trackers[exp_key].update(metrics)
            for m_key in ['KLD', 'SIM', 'NSS']:
                df_fin.at[index, f"{exp_key}_EgoOnly_{m_key}"] = metrics[m_key]

            trackers[exp_key].print_metrics(metrics, f"[Ego] {filename}")
        print(f"selected token exo :{adaptive_refine_exo} ego : {adaptive_refine_ego}")
# -------------------------------------------------------------------------
    # [VISUALIZATION] 3-Metric Display & Exo Filename Version
    # -------------------------------------------------------------------------
    fig, axes = plt.subplots(2, 8, figsize=(32, 12)) # í…ìŠ¤íŠ¸ ê³µê°„ í™•ë³´ë¥¼ ìœ„í•´ ë†’ì´ë¥¼ 12ë¡œ ì¡°ì •
    
    # --- ê³µí†µ ì›ë³¸ ë° GT ë°°ì¹˜ ---
    axes[0, 0].imshow(orig_img)
    axes[0, 0].set_title(f"Ego Image\n{action}_{object_name}", fontsize=12, fontweight='bold')
    axes[0, 0].axis('off')
    
    axes[1, 0].imshow(orig_img)
    axes[1, 0].axis('off')

    # --- Exo Context ì´ë¯¸ì§€ ë° íŒŒì¼ëª… í‘œì‹œ ---
    exo_img_plot = Image.open(exo_path).convert("RGB")
    exo_filename = os.path.basename(exo_path) # ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ
    axes[0, 1].imshow(exo_img_plot)
    # íŒŒì¼ëª…ì´ ê¸¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í°íŠ¸ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ê³  ì¤„ë°”ê¿ˆ ì ìš© ê°€ëŠ¥
    axes[0, 1].set_title(f"Exo Context\n({exo_filename})", fontsize=10, color='darkgreen')
    axes[0, 1].axis('off')

    # --- GT ë°°ì¹˜ ---
    if gt_map is not None:
        axes[1, 1].imshow(gt_map, cmap='gray')
        axes[1, 1].set_title("Ground Truth", fontsize=12)
        axes[1, 1].axis('off')
    else:
        axes[1, 1].axis('off')

    # --- ì‹¤í—˜ ê²°ê³¼ ë°°ì¹˜ (Exp 1-6 & 7-12) ---
    titles = ["LastIn", "AvgOut", "Top1", "Top1+Obj", f"Top1+Adapt_exo:{adaptive_refine_exo}_ego:{adaptive_refine_ego}", f"Top1+PLSP({PLSP_name})"]
    for j in range(6):
        # 1. Context Row (Top)
        ax_ctx = axes[0, j+2]
        key_ctx = f'Exp{j+1}'
        ax_ctx.imshow(maps_context[key_ctx], cmap='jet')
        ax_ctx.set_title(f"Ctx_{titles[j]}", fontsize=11, fontweight='bold')
        ax_ctx.axis('off')
        
        # 2. Ego-Only Row (Bottom)
        ax_ego = axes[1, j+2]
        key_ego = f'Exp{j+7}'
        ax_ego.imshow(maps_ego_only[f'Exp{j+1}'], cmap='jet')
        ax_ego.set_title(f"Ego_{titles[j]}", fontsize=11, fontweight='bold')
        ax_ego.axis('off')
        
        # 3. ë©”íŠ¸ë¦­ í‘œì‹œ (KLD, SIM, NSS)
        if gt_map is not None:
            # ë°ì´í„°í”„ë ˆì„ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
            k_c = df_fin.at[index, f"{key_ctx}_Context_KLD"]
            s_c = df_fin.at[index, f"{key_ctx}_Context_SIM"]
            n_c = df_fin.at[index, f"{key_ctx}_Context_NSS"]
            
            k_e = df_fin.at[index, f"{key_ego}_EgoOnly_KLD"]
            s_e = df_fin.at[index, f"{key_ego}_EgoOnly_SIM"]
            n_e = df_fin.at[index, f"{key_ego}_EgoOnly_NSS"]

            # ë©”íŠ¸ë¦­ í…ìŠ¤íŠ¸ êµ¬ì„±
            metric_text_ctx = f"K: {k_c:.2f}\nS: {s_c:.2f}\nN: {n_c:.2f}"
            metric_text_ego = f"K: {k_e:.2f}\nS: {s_e:.2f}\nN: {n_e:.2f}"

            # í…ìŠ¤íŠ¸ ë°•ìŠ¤ ë°°ì¹˜
            ax_ctx.text(0.5, -0.05, metric_text_ctx, transform=ax_ctx.transAxes, 
                        ha='center', va='top', fontsize=10, color='blue', fontweight='semibold',
                        bbox=dict(facecolor='white', alpha=0.8, edgecolor='lightgray', boxstyle='round,pad=0.3'))
            
            ax_ego.text(0.5, -0.05, metric_text_ego, transform=ax_ego.transAxes, 
                        ha='center', va='top', fontsize=10, color='red', fontweight='semibold',
                        bbox=dict(facecolor='white', alpha=0.8, edgecolor='lightgray', boxstyle='round,pad=0.3'))

    # ë ˆì´ì•„ì›ƒ ì¡°ì •
    plt.tight_layout()
    # í…ìŠ¤íŠ¸ ë°•ìŠ¤ê°€ í•˜ë‹¨ í”„ë ˆì„ì— ê°€ë ¤ì§€ì§€ ì•Šë„ë¡ ì¶©ë¶„í•œ ì—¬ë°± í™•ë³´
    plt.subplots_adjust(bottom=0.18, hspace=0.4) 
    
    # ê²°ê³¼ ì €ì¥
    save_name = f"{action}_{object_name}_{filename.split('.')[0]}.png"
    save_path = os.path.join(VIS_DIR, save_name)
    plt.savefig(save_path, bbox_inches='tight', dpi=150)
    plt.close(fig)

    # -------------------------------------------------------------------------
    # [CLEANUP] Memory & Intermediate Saving
    # -------------------------------------------------------------------------
    if index % 50 == 0:
        df_fin.to_pickle(SAVE_FILENAME)
        print(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {SAVE_FILENAME} ({index}/{len(df_fin)})")
    
    # ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ
    del out_ctx, out_ego, maps_context, maps_ego_only, gt_map
    torch.cuda.empty_cache()
    gc.collect()

# ìµœì¢… ì €ì¥ ë° í‰ê·  ì¶œë ¥
df_fin.to_pickle(SAVE_FILENAME)
print("\n" + "="*50 + "\nğŸ‰ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!\n" + "="*50)