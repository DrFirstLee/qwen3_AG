{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a81672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bongo/anaconda3/envs/qwen3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading Qwen/Qwen3-VL-2B-Instruct for selection...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "sys.path.append(\"/home/bongo/porter_notebook/research/qwen3\")\n",
    "\n",
    "# config Î∞è file_managingÏóêÏÑú ÌïÑÏöîÌïú Î≥ÄÏàò/Ìï®Ïàò ÏûÑÌè¨Ìä∏\n",
    "from config import AGD20K_PATH, model_name\n",
    "from file_managing import make_input_image\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1. Î™®Îç∏ Î°úÎî©\n",
    "# ------------------------------------------------------\n",
    "# SDPAÎ•º ÎÅÑÎ©¥ Î©îÎ™®Î¶¨Î•º Ï¢Ä Îçî Ïì∞ÏßÄÎßå Attention MapÏùÄ ÌôïÏã§Ìûà ÎÇòÏòµÎãàÎã§.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTORCH_ENABLE_SDPA\"] = \"0\"  # 1 -> 0 Î≥ÄÍ≤Ω\n",
    "os.environ[\"PYTORCH_ENABLE_FLASH_SDP\"] = \"0\"\n",
    "os.environ[\"PYTORCH_ENABLE_MEM_EFFICIENT_SDP\"] = \"0\"\n",
    "\n",
    "print(f\"ü§ñ Loading {model_name} for selection...\")\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map=\"cuda\", \n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda4b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path, target_action, object_name = \"/home/DATA/AGD20K/Seen/trainset/exocentric/carry/surfboard/carry_surfboard_001939.jpg\", \"carry\", \"surfboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1c7a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carry'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------\n",
    "# 1. Ï†ÑÏ≤òÎ¶¨: ÌïµÏã¨ ÎèôÏÇ¨ Ï∂îÏ∂ú\n",
    "# -----------------------------------------------------------\n",
    "# ÏÇ¨Ï†Ñ ÏóÜÏù¥ Îã®Ïàú splitÎßå ÏÇ¨Ïö©\n",
    "core_action = target_action.split('_')[0].lower() \n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 2. ÏßàÎ¨∏: ÎèôÏÇ¨ ÎÇòÏó¥ Ïú†ÎèÑ\n",
    "# -----------------------------------------------------------\n",
    "query = f\"What actions is the person doing with the {object_name}? list all the possible verbs. Only list the verbs.\"\n",
    "\n",
    "\n",
    "image_base64 = make_input_image(str(image_path))\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": f\"data:image/jpeg;base64,{image_base64}\"},\n",
    "        {\"type\": \"text\", \"text\": query}\n",
    "    ]}\n",
    "]\n",
    "inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Vision Token Ïù∏Îç±Ïä§ Ï∞æÍ∏∞\n",
    "input_ids_list = inputs.input_ids[0].tolist()\n",
    "vis_start_id = processor.tokenizer.convert_tokens_to_ids(\"<|vision_start|>\")\n",
    "vis_end_id = processor.tokenizer.convert_tokens_to_ids(\"<|vision_end|>\")\n",
    "\n",
    "vis_start_idx = input_ids_list.index(vis_start_id)\n",
    "vis_end_idx = input_ids_list.index(vis_end_id)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=30, output_attentions=True, return_dict_in_generate=True)\n",
    "    \n",
    "output_ids = generated_ids.sequences[0][inputs.input_ids.shape[1]:]\n",
    "full_text = processor.decode(output_ids, skip_special_tokens=True).lower()\n",
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff53e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 73765, 151645], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40270bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç [Debug Indices] Start: 3, End: 965\n",
      "üîç [Debug Length] Image Tokens Count: 961\n",
      "Token 0: carry  / :0.0\n",
      "FIN : 0.0 carry\n",
      "0.0 carry\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# 5. Targeted Scoring (Safety Net Ï∂îÍ∞Ä)\n",
    "# -----------------------------------------------------------\n",
    "total_vis_score = 0.0     # Îß§Ïπ≠Îêú ÌÜ†ÌÅ∞Îì§Ïùò Ìï©\n",
    "matched_count = 0         # Îß§Ïπ≠Îêú ÌÜ†ÌÅ∞ Ïàò\n",
    "\n",
    "all_tokens_energy_sum = 0.0 # (Safety NetÏö©) Ï†ÑÏ≤¥ ÌÜ†ÌÅ∞ ÏóêÎÑàÏßÄ Ìï©\n",
    "valid_token_count = 0       # (Safety NetÏö©) Ï†ÑÏ≤¥ Ïú†Ìö® ÌÜ†ÌÅ∞ Ïàò\n",
    "\n",
    "for i, token_id in enumerate(output_ids):\n",
    "    token_str = processor.decode([token_id], skip_special_tokens=True).lower().strip()\n",
    "    \n",
    "    if not token_str: continue # Í≥µÎ∞± ÌÜ†ÌÅ∞ Î¨¥Ïãú\n",
    "\n",
    "    # --- [ÏóêÎÑàÏßÄ Í≥ÑÏÇ∞] ---\n",
    "    # Îß§Ïπ≠ Ïó¨Î∂ÄÏôÄ ÏÉÅÍ¥ÄÏóÜÏù¥ ÏùºÎã® ÌòÑÏû¨ ÌÜ†ÌÅ∞Ïùò ÏóêÎÑàÏßÄÎ•º Í≥ÑÏÇ∞Ìï¥Îë°ÎãàÎã§.\n",
    "    token_energy = 0.0\n",
    "    # ... Ïù∏Îç±Ïä§ Ï∞æÎäî Î∂ÄÎ∂Ñ ÏßÅÌõÑ ...\n",
    "    print(f\"üîç [Debug Indices] Start: {vis_start_idx}, End: {vis_end_idx}\")\n",
    "    print(f\"üîç [Debug Length] Image Tokens Count: {vis_end_idx - vis_start_idx - 1}\")\n",
    "\n",
    "    for layer_attn in generated_ids.attentions[i]:\n",
    "        # Vision Token ÏòÅÏó≠Îßå Ïä¨ÎùºÏù¥Ïã±ÌïòÏó¨ Ìï©ÏÇ∞\n",
    "        vision_attn = layer_attn[0, :, 0, vis_start_idx+1 : vis_end_idx]\n",
    "        token_energy += vision_attn.float().sum().item()\n",
    "        # print(f\"token_energy : {token_energy}\")\n",
    "    \n",
    "    # Safety NetÏùÑ ÏúÑÌï¥ Ï†ÑÏ≤¥ ÎàÑÏ†Å\n",
    "    all_tokens_energy_sum += token_energy\n",
    "    valid_token_count += 1\n",
    "\n",
    "    # --- [ÌÜ†ÌÅ∞ Îß§Ïπ≠ ÌôïÏù∏] ---\n",
    "    # 1. Core ActionÏù¥ TokenÏùÑ Ìè¨Ìï® (Ïòà: carry >= car) -> BPE ÌååÌé∏Ìôî ÎåÄÏùë\n",
    "    # 2. TokenÏù¥ Core ActionÏùÑ Ìè¨Ìï® (Ïòà: carrying >= carry) -> Î≥ÄÌòï ÎåÄÏùë\n",
    "    if (core_action in token_str) or (token_str in core_action and len(token_str) > 1): \n",
    "        # print(\"good\")\n",
    "        # len > 1 Ï°∞Í±¥: 'c', 'a' Í∞ôÏùÄ ÎÑàÎ¨¥ ÏßßÏùÄ ÌååÌé∏Ïù¥ ÏóÑÌïú Îã®Ïñ¥Ïóê Îß§Ïπ≠ÎêòÎäî Í≤É Î∞©ÏßÄ\n",
    "        total_vis_score += token_energy\n",
    "        matched_count += 1\n",
    "        # print(a)\n",
    "    print(f\"Token {i}: {token_str}  / :{total_vis_score}\")\n",
    "# -----------------------------------------------------------\n",
    "# [Í≤∞Í≥º Î∞òÌôò Î°úÏßÅ]\n",
    "# -----------------------------------------------------------\n",
    "if matched_count > 0:\n",
    "    # 1. Ï†ïÌôïÌûà Îß§Ïπ≠Îêú ÌÜ†ÌÅ∞Ïù¥ ÏûàÏúºÎ©¥ Í∑∏ Ï†êÏàò ÏÇ¨Ïö© (Best)\n",
    "    final_score = total_vis_score / matched_count\n",
    "    print(\"FIN :\",  final_score, full_text )\n",
    "else:\n",
    "    # 2. [Safety Net] Îß§Ïπ≠Îêú ÌÜ†ÌÅ∞ÏùÄ ÏóÜÏßÄÎßå, full_textÏóêÎäî Ï†ïÎãµÏù¥ ÏûàÏóàÏùå!\n",
    "    # ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Î¨∏Ï†úÎ°ú ÌåêÎã®ÌïòÍ≥†, Ï†ÑÏ≤¥ Î¨∏Ïû•Ïùò ÌèâÍ∑† ÏóêÎÑàÏßÄÎ•º Î∞òÌôò (Fallback)\n",
    "    if valid_token_count > 0:\n",
    "        final_score = all_tokens_energy_sum / valid_token_count\n",
    "        print(\"FIN2 :\",  final_score, full_text )\n",
    "    else:\n",
    "        print(\"FIN3 :\",  0.0, full_text )# ÌÜ†ÌÅ∞Ïù¥ ÏóÜÏúºÎ©¥ 0Ï†ê\n",
    "\n",
    "print( final_score, full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701c56e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_attn[0, :, 0, : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9fc13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Current Attention Implementation: Qwen3VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"image_token_id\": 151655,\n",
      "  \"model_type\": \"qwen3_vl\",\n",
      "  \"text_config\": {\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 151643,\n",
      "    \"dtype\": \"bfloat16\",\n",
      "    \"eos_token_id\": 151645,\n",
      "    \"head_dim\": 128,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 6144,\n",
      "    \"max_position_embeddings\": 262144,\n",
      "    \"model_type\": \"qwen3_vl_text\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 28,\n",
      "    \"num_key_value_heads\": 8,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_scaling\": {\n",
      "      \"mrope_interleaved\": true,\n",
      "      \"mrope_section\": [\n",
      "        24,\n",
      "        20,\n",
      "        20\n",
      "      ],\n",
      "      \"rope_type\": \"default\"\n",
      "    },\n",
      "    \"rope_theta\": 5000000,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 151936\n",
      "  },\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"deepstack_visual_indexes\": [\n",
      "      5,\n",
      "      11,\n",
      "      17\n",
      "    ],\n",
      "    \"depth\": 24,\n",
      "    \"dtype\": \"bfloat16\",\n",
      "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 1024,\n",
      "    \"in_channels\": 3,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"model_type\": \"qwen3_vl\",\n",
      "    \"num_heads\": 16,\n",
      "    \"num_position_embeddings\": 2304,\n",
      "    \"out_hidden_size\": 2048,\n",
      "    \"patch_size\": 16,\n",
      "    \"spatial_merge_size\": 2,\n",
      "    \"temporal_patch_size\": 2\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"üîç Current Attention Implementation: {model.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf843b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd8b62c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_attn[0, :, 0, :].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dce32e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 993])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(layer_attn[0, :, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8ec7d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94afeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
